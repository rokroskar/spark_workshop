{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Spark \n",
    "\n",
    "We have discussed some [basic features of Spark](Spark_intro.ipynb) -- now we'll try to actually use the framework for some basic operations. \n",
    "\n",
    "In particular, this notebook will walk you through some of the basic [Spark RDD methods](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD). As you'll see, there is a lot more to it than `map` and `reduce`. Some of the very useful operations include:\n",
    "\n",
    "* `groupByKey`\n",
    "* `reduceByKey`\n",
    "* `sortBy`\n",
    "* `distinct`\n",
    "\n",
    "We will explore the concept of \"lineage\" in Spark RDDs and construct some simple key-value pair RDDs to write our first Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting up the `SparkContext` locally\n",
    "\n",
    "The most lightweight way of playing around with Spark is to run the whole Spark runtime on a single (local) machine. \n",
    "\n",
    "First, we need to do a few lines of setup (we can later move these to a startup script of some sort) and then we start the `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ['SPARK_HOME'] = '/Users/rok/spark'\n",
    "sys.path.insert(0, '/Users/rok/spark/python/')\n",
    "sys.path.insert(0, '/Users/rok/spark/python/lib/py4j-0.8.2.1-src.zip')\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x109e1e790>\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(master='local[4]')\n",
    "print sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hurrah! We have a Spark Context! Now lets get some data in there just to see if it works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements:  100\n",
      "Sum and mean:  4950 49.5\n"
     ]
    }
   ],
   "source": [
    "data = xrange(100)\n",
    "rdd = sc.parallelize(data)\n",
    "print 'Number of elements: ', rdd.count()\n",
    "print 'Sum and mean: ', rdd.sum(), rdd.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you look at your console, you will see *a lot* of output -- Spark is reporting all the stages of execution and can become rather verbose. Initially it's useful to inspect this output just to see what's going on and to see when issues arise. Later on we'll see how to quiet it down. \n",
    "\n",
    "In addition, each Spark application runs its own dedicated Web UI, accessible by default at `driver:4040`. In this case this is http://localhost:4040. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
