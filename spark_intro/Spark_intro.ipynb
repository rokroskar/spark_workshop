{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting started with...\n",
    "\n",
    "![Apache Spark](http://spark.apache.org/images/spark-logo.png)\n",
    "\n",
    "In the [previous session](../intro/Spark_workshop_Introduction.ipynb) we discussed some basics of the map/reduce programming paradigm. \n",
    "\n",
    "Now, we'll see how we can \"scale up\" these ideas to distribute the workload among many machines.\n",
    "\n",
    "For this, we will use [Spark](http://spark.apache.org), a distributed computing framework that fits nicely into \n",
    "the [Apache Hadoop](http://hadoop.apache.org) ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Data Abstraction -> the Resilient Distributed Dataset\n",
    "\n",
    "An RDD is the essential building block of every Spark application. \n",
    "\n",
    "* keeps track of data distribution across the workers  \n",
    "* provides an interface to the user to access and operate on the data\n",
    "* can rebuild data upon failure\n",
    "* keeps track of lineage \n",
    "\n",
    "** As a Spark user, you write applications that feed data into RDDs and subsequently transform them into something useful **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Basic RDD diagram](https://raw.githubusercontent.com/rokroskar/spark_workshop/master/figs/basic_rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Architecture Overview\n",
    "\n",
    "A very abbreviated and naive description of how Spark works: \n",
    "\n",
    "The runtime system consists of a **driver**, **executors** and **workers**. The driver coordinates the work to be done and keeps track of tasks, collects metrics about the tasks (disk IO, memory, etc.), and communicates with the executors. The executors receive tasks to be done from the driver and distribute them among the workers that they control. Bsaically, you can think of an executor as a *node* and a worker as a *core* on that node. \n",
    "\n",
    "The user's access point to this Spark universe is the **Spark Context** which provides an interface to generate RDDs. \n",
    "\n",
    "** The only point of contact with the Spark \"universe\" is the Spark Context and the RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark Universe](https://raw.githubusercontent.com/rokroskar/spark_workshop/master/figs/spark_universe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flexibility of Spark runtime\n",
    "\n",
    "A few points before we get our feet wet with doing some basic data massaging in Spark. \n",
    "\n",
    "The spark runtime can be deployed on: \n",
    "* a single machine (local)\n",
    "* a set of pre-defined machines (stand-alone)\n",
    "* a dedicated Hadoop-aware scheduler (YARN/Mesos)\n",
    "* \"cloud\", e.g. Amazon EC2 \n",
    "\n",
    "The development workflow is that you start small (local) and scale up to one of the other solutions, depending on your needs and resources. \n",
    "\n",
    "In addition, you can run applications on any of these platforms either\n",
    "\n",
    "* interactively through a shell (or an ipython notebook as we'll see)\n",
    "* batch mode \n",
    "\n",
    "** Often, you don't need to change *any* code to go between these methods of deployment! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##RDD transformations/actions\n",
    "\n",
    "Once an RDD is created, it is **immutable** - it can only be modified via a *transformation*\n",
    "\n",
    "Transformations include: \n",
    "* `map` -- the most basic component of map/reduce with 1:1 correspondence to original data\n",
    "* `flatMap` -- returns a number of items different from the original data\n",
    "* `filter` -- only keep those elements for which the filter function evaluates to `True`\n",
    "* `distinct` -- only retain the unique elements of the entire RDD\n",
    "* `reduceByKey` -- group elements by key and keep the data distributed \n",
    "* `mapPartitions` -- similar to `map` but done on a per-partition basis\n",
    "\n",
    "Transformations are \"lazy\" evaluated - the system keeps a lineage graph that is only executed once an *action* is performed. \n",
    "\n",
    "Actions include: \n",
    "* `collect` -- pulls all elements of the RDD to the driver (often a bad idea!!)\n",
    "* `collectAsMap` -- like `collect` but returns a dictionary to the driver which makes it easy to lookup the keys \n",
    "* `reduce` -- reduces the entire RDD to a single value\n",
    "* `countByKey`/`countByValue`\n",
    "* `take` -- yields a desired number of items to the driver\n",
    "* `first` -- returns the first element of the RDD to the driver -- very useful for inspecting the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lineage\n",
    "\n",
    "* When an RDD is transformed, this **transformation** is not automatically carried out. \n",
    "* Instead, the system remembers how to get from one RDD to another and only executes whatever is needed for the **action** that is being done. \n",
    "\n",
    "## Partitioning\n",
    "\n",
    "* data of each RDD is partitioned and each partition is assigned to an executor\n",
    "* each partition in a transformation results in a task\n",
    "* there may be many more tasks than cores in the system, which allows for good utilization by fine-graining the overall load."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
