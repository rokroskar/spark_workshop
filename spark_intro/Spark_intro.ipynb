{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting started with...\n",
    "\n",
    "![Apache Spark](http://spark.apache.org/images/spark-logo.png)\n",
    "\n",
    "In the [previous session](../intro/Spark_workshop_Introduction.ipynb) we discussed some basics of the map/reduce programming paradigm. \n",
    "\n",
    "Now, we'll see how we can \"scale up\" these ideas to distribute the workload among many machines.\n",
    "\n",
    "For this, we will use [Spark](http://spark.apache.org), a distributed computing framework that fits nicely into \n",
    "the [Apache Hadoop](http://hadoop.apache.org) ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Data Abstraction -> the Resilient Distributed Dataset\n",
    "\n",
    "An RDD is the essential building block of every Spark application. \n",
    "\n",
    "* keeps track of data distribution across the workers  \n",
    "* provides an interface to the user to access and operate on the data\n",
    "* can rebuild data upon failure\n",
    "\n",
    "** As a Spark user, you write applications that feed data into RDDs and subsequently transform them into something useful **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Basic RDD diagram](https://raw.githubusercontent.com/rokroskar/spark_workshop/master/figs/basic_rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Architecture Overview\n",
    "\n",
    "A very abbreviated and naive description of how Spark works: \n",
    "\n",
    "The runtime system consists of a **driver**, **executors** and **workers**. The driver coordinates the work to be done and keeps track of tasks, collects metrics about the tasks (disk IO, memory, etc.), and communicates with the executors. The executors receive tasks to be done from the driver and distribute them among the workers that they control. Bsaically, you can think of an executor as a *node* and a worker as a *core* on that node. \n",
    "\n",
    "The user's access point to this Spark universe is the **Spark Context** which provides an interface to generate RDDs. \n",
    "\n",
    "** The only point of contact with the Spark \"universe\" is the Spark Context and the RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark Universe](https://raw.githubusercontent.com/rokroskar/spark_workshop/master/figs/spark_universe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flexibility of Spark runtime\n",
    "\n",
    "A few points before we get our feet wet with doing some basic data massaging in Spark. \n",
    "\n",
    "The spark runtime can be deployed on: \n",
    "* a single machine (local)\n",
    "* a set of pre-defined machines (stand-alone)\n",
    "* a dedicated Hadoop-aware scheduler (YARN/Mesos)\n",
    "* \"cloud\", e.g. Amazon EC2 \n",
    "\n",
    "The development workflow is that you start small (local) and scale up to one of the other solutions, depending on your needs and resources. \n",
    "\n",
    "In addition, you can run applications on any of these platforms either\n",
    "\n",
    "* interactively through a shell (or an ipython notebook as we'll see)\n",
    "* batch mode \n",
    "\n",
    "** Often, you don't need to change *any* code to go between these methods of deployment! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting up the `SparkContext` locally\n",
    "\n",
    "The most lightweight way of playing around with Spark is to run the whole Spark runtime on a single (local) machine. \n",
    "\n",
    "First, we need to do a few lines of setup (we can later move these to a startup script of some sort) and then we start the `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ['SPARK_HOME'] = '/Users/rok/spark'\n",
    "sys.path.insert(0, '/Users/rok/spark/python/')\n",
    "sys.path.insert(0, '/Users/rok/spark/python/lib/py4j-0.8.2.1-src.zip')\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master='local[4]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hurrah! We have a Spark Context! Now lets get some data in there just to see if it works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements:  1000000\n",
      "Sum and mean:  500178.597714 0.500178597714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.rand(1000000)\n",
    "rdd = sc.parallelize(data)\n",
    "print 'Number of elements: ', rdd.count()\n",
    "print 'Sum and mean: ', rdd.sum(), rdd.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you look at your console, you will see *a lot* of output -- Spark is reporting all the stages of execution and can become rather verbose. Initially it's useful to inspect this output just to see what's going on and to see when issues arise. Later on we'll see how to quiet it down. \n",
    "\n",
    "In addition, each Spark application runs its own dedicated Web UI, accessible by default at `driver:4040`. In this case this is http://localhost:4040. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
