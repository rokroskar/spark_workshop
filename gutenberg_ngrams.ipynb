{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Spark Context: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ['SPARK_HOME'] = '/Users/rok/spark'\n",
    "sys.path.insert(0, '/Users/rok/spark/python/')\n",
    "sys.path.insert(0, '/Users/rok/spark/python/lib/py4j-0.8.2.1-src.zip')\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x10aa01790>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executor.memory', '2g')\n",
    "conf.set('spark.driver.memory', '4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master = \"local[4]\", conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a key-value RDD of book metadata and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data into spark from a collection of local files is a very common task. A useful pattern to keep in mind is the following: \n",
    "\n",
    "1. make a list of filenames and distribute it among the workers\n",
    "3. \"map\" each filename to the data you want to get out\n",
    "4. now you are left with the RDD of raw data distributed among the workers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`gutenberg_cleanup`](gutenberg_cleanup.py) module contains two functions that can help with this: `get_text` and `get_metadata`.\n",
    "\n",
    "They pretty much do the obvious: \n",
    "\n",
    "`get_metadata` returns a metadata object with various useful fields that will be used to create a unique key for each book\n",
    "\n",
    "`get_text` returns the raw text extracted from HTML, cleaned of tags and punctuation and converted to lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Distributing the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of books:  3206\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "flist = glob.glob('/Users/rok/python_src/gutenberg/dl-cache/*html')\n",
    "print 'number of books: ', len(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_rdd = sc.parallelize(flist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rok/python_src/gutenberg/dl-cache/1000.html',\n",
       " '/Users/rok/python_src/gutenberg/dl-cache/1001.html',\n",
       " '/Users/rok/python_src/gutenberg/dl-cache/1002.html',\n",
       " '/Users/rok/python_src/gutenberg/dl-cache/1003.html',\n",
       " '/Users/rok/python_src/gutenberg/dl-cache/1004.html']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Map the filenames to metadata, text key-value pairs\n",
    "\n",
    "Use the `get_text` and `get_metadata` functions to construct a key,value pair RDD, where `key` is composed of a string:\n",
    "\n",
    "`title||last_name||first_name||birth_year||death_year`\n",
    "\n",
    "Use \"||\" as a delimiter. \n",
    "\n",
    "For the `value` of each `key`,`value` pair use the raw text returned by `get_text`. \n",
    "\n",
    "Hint: in Python, there are many ways to make a string, but a pretty easy one is like this: \n",
    "\n",
    "    \"bla_%s\"%(var)\"\n",
    "\n",
    "where `var` matches the `%s` and is a variable that can be converted to a string. You can include more `%s` (or `%d`, `%f` etc) and more variables in the tuple that follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gutenberg_cleanup import get_metadata, get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd = (files_rdd.map(lambda filename: \n",
    "                         (get_metadata(filename), get_text(filename)))\n",
    "                     .map(lambda (meta, text): (\"%s||%s||%s||%s||%s\"%(meta.title,meta.first_name,meta.last_name,meta.birth_year,meta.death_year), text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we don't have to constantly re-load the data off disk, lets cache this RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3130"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the first set of keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- No Title -||None||None||None||None',\n",
       " u\"Divine Comedy, Longfellow's Translation, Hell||None||Dante Alighieri||1265||1321\",\n",
       " u\"Divine Comedy, Longfellow's Translation, Purgatory||None||Dante Alighieri||1807||1882\",\n",
       " u\"Divine Comedy, Longfellow's Translation, Paradise||None||Dante Alighieri||1265||1321\",\n",
       " u\"Divine Comedy, Longfellow's Translation, Complete||None||Dante Alighieri||1807||1882\",\n",
       " u\"Divine Comedy, Cary's Translation, Hell||None||Dante Alighieri||1772||1844\",\n",
       " u\"Divine Comedy, Cary's Translation, Purgatory||None||Dante Alighieri||1265||1321\",\n",
       " u\"Divine Comedy, Cary's Translation, Paradise||None||Dante Alighieri||1265||1321\",\n",
       " u\"Divine Comedy, Cary's Translation, Complete||None||Dante Alighieri||1772||1844\",\n",
       " u'The Hacker Crackdown: Law and Disorder on the Electronic Frontier||Bruce||Sterling||1954||None',\n",
       " u'The First Men in the Moon||H. G. (Herbert George)||Wells||1866||1946',\n",
       " u'The Lure of the Dim Trails||B. M.||Bower||1874||1940',\n",
       " u'The Oregon Trail: Sketches of Prairie and Rocky-Mountain Life||Francis||Parkman||1823||1893',\n",
       " u'On the Improvement of the Understanding||Benedictus de||Spinoza||1632||1677',\n",
       " u'The Soul of Man under Socialism||Oscar||Wilde||1854||1900',\n",
       " u'Poems by Currer, Ellis, and Acton Bell||Emily||Bront\\xeb||1818||1848',\n",
       " u\"The Tragedy of Pudd'nhead Wilson||Mark||Twain||1835||1910\",\n",
       " u'Sword Blades and Poppy Seed||Amy||Lowell||1874||1925',\n",
       " u'The Congo, and Other Poems||Vachel||Lindsay||1879||1931',\n",
       " u'Walking||Henry David||Thoreau||1817||1862',\n",
       " u'Bleak House||Charles||Dickens||1812||1870',\n",
       " u'The Wrecker||Lloyd||Osbourne||1868||1947',\n",
       " u'Essays; Political, Economical, and Philosophical \\u2014 Volume 1||Graf von Rumford||Rumford||1753||1814',\n",
       " u'The Diary of a Nobody||Weedon||Grossmith||1852||1919',\n",
       " u'The Lone Star Ranger: A Romance of the Border||Zane||Grey||1872||1939',\n",
       " u'The Professor||Charlotte||Bront\\xeb||1816||1855',\n",
       " u'The Night-Born||Jack||London||1876||1916',\n",
       " u'Around the World in Eighty Days||Jules||Verne||1828||1905',\n",
       " u'The Cavalier Songs and Ballads of England from 1642 to 1684||None||None||1814||1889',\n",
       " u'Charmides, and Other Poems||Oscar||Wilde||1854||1900',\n",
       " u'The Pupil||Henry||James||1843||1916',\n",
       " u\"Rose o' the River||Kate Douglas Smith||Wiggin||1856||1923\",\n",
       " u'Poems||Wilfred||Owen||1893||1918',\n",
       " u'The Man Against the Sky: A Book of Poems||Edwin Arlington||Robinson||1869||1935',\n",
       " u'Joe Wilson and His Mates||Henry||Lawson||1867||1922',\n",
       " u'The Life of John Bunyan||Edmund||Venables||1819||1895',\n",
       " u'Style||Sir Raleigh||Raleigh||1861||1922',\n",
       " u'Missionary Travels and Researches in South Africa||David||Livingstone||1813||1873',\n",
       " u'Inaugural Address of Franklin Delano Roosevelt\\r||Franklin D. (Franklin Delano)||Roosevelt||1882||1945',\n",
       " u'The Three Taverns: A Book of Poems||Edwin Arlington||Robinson||1869||1935',\n",
       " u\"Shakespeare's Sonnets||William||Shakespeare||1564||1616\",\n",
       " u'A Reading of Life, with Other Poems||George||Meredith||1828||1909',\n",
       " u'The Story of Evolution||Joseph||McCabe||1867||1955',\n",
       " u\"Extract from Captain Stormfield's Visit to Heaven||Mark||Twain||1835||1910\",\n",
       " u'Venus and Adonis||William||Shakespeare||1564||1616',\n",
       " u'God, the Invisible King||H. G. (Herbert George)||Wells||1866||1946',\n",
       " u'The New Machiavelli||H. G. (Herbert George)||Wells||1866||1946',\n",
       " u'The Ruling Passion: Tales of Nature and Human Nature||Henry||Van Dyke||1852||1933',\n",
       " u'Vanished Arizona: Recollections of the Army Life by a New England Woman||Martha||Summerhayes||1844||1926',\n",
       " u'Persuasion||Jane||Austen||1775||1817']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.keys().take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that we're going to have to do some quality control here. For example, we probably don't want books with \"None\" as either of the author names, and likewise we have to have the birth date in order to be able to create a time series out of the data in the end. \n",
    "\n",
    "Construct an RDD, as above, except that you filter out all elements that lack a value for `title`, `first_name`, `last_name`, or `birth_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_rdd = (files_rdd.map(lambda filename: get_metadata(filename))\n",
    "                         .filter(lambda metadata: metadata.title is not None and \n",
    "                                                  metadata.first_name is not None and\n",
    "                                                  metadata.last_name is not None and \n",
    "                                                  metadata.birth_year is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 11, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 414, in dumps\n    return pickle.dumps(obj, protocol)\nRuntimeError: maximum recursion depth exceeded in cmp\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-6551f56582cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rok/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \"\"\"\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rok/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rok/spark/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[0;32m--> 881\u001b[0;31m                                           allowLocal)\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rok/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rok/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 11, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 267, in dump_stream\n    bytes = self.serializer.dumps(vs)\n  File \"/Users/rok/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 414, in dumps\n    return pickle.dumps(obj, protocol)\nRuntimeError: maximum recursion depth exceeded in cmp\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:70)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
     ]
    }
   ],
   "source": [
    "filtered_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
