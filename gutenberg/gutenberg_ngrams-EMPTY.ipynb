{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg N-Grams\n",
    "\n",
    "In this notebook, we will quantitatively explore the text of the [Gutenberg E-Books Project](https://www.gutenberg.org/), a free repository of e-books that are in the public domain. All of the English and German books have been downloaded for this tutorial and a small python package has been made available that allows you to easily parse the text and the associated metadata. \n",
    "\n",
    "But before we get to using the data, we need to take care of a few setup issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the spark environment can be a bit of a trial and error procedure. Often you'll need to configure settings (in particular dealing with memory) to fit your cluster and your particular application. Below, we will specify a few of the most important ones -- but you can see the full list in the [Spark Configuration guide](http://spark.apache.org/docs/latest/configuration.html) and if you are using YARN there are critical options also listed under the [YARN deployment guide](http://spark.apache.org/docs/latest/running-on-yarn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python environment setup\n",
    "\n",
    "First, we need to make sure that the `pyspark` libraries are accessible in this notebook. To do this, we can add them to the library search path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, glob\n",
    "try: \n",
    "    spark_home = os.environ['SPARK_HOME']\n",
    "except KeyError: \n",
    "    raise KeyError(\"You must specify the SPARK_HOME environment variable -- is the Spark module loaded?\")\n",
    "\n",
    "python_lib_path = glob.glob('%s/python/lib/py4j*zip'%spark_home)[0]\n",
    "sys.path.insert(0,spark_home+'/python')\n",
    "sys.path.insert(0,python_lib_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing `SparkConf` \n",
    "When starting the Spark runtime through the notebook or inside a script (i.e. when not calling one of the spark scripts like `spark_submit`), you can create a `SparkConf` object that allows you to set up the runtime. This is quite convenient and much more clean and readable than specifying the options on the commandline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the number of executors and cores into variables so we can refer to it later\n",
    "num_execs = 20\n",
    "exec_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing the SparkConf\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor options\n",
    "\n",
    "The full list of options is very long, but the basic ones you'll *always* want to at least think about are ones pertaining to the basic configuration of the executors: number of executors, memory per executor, and number of cores per executor. \n",
    "\n",
    "A few notes about the memory configuration: the `spark.executor.memory` should not be set to the total memory of the node. Some memory is needed for the OS (including HDFS and other services), and still more is required for the Spark overhead. So in our case here, we have 16 Gb of memory per node but can only use around 12 Gb of this for the executors. Since we need to leave room for 10% YARN overhead, we specify 9 Gb here to be safe. If your executors start dying off for strange reasons, try reducing the memory here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf.set('spark.executor.memory', '9g')\n",
    "conf.set('spark.executor.instances', str(num_execs))\n",
    "conf.set('spark.executor.cores', str(exec_cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory configuration\n",
    "\n",
    "Two other key memory options exist, specifying the amount of executor memory reserved for *cached* data and for *shuffle* data. Depending on what your application is doing, you may need more of one or the other. For example, if you are running a lot of iterative operations on a large dataset, you probably want a good amount of memory for RDD caching. On the other hand, if you are doing lots of expensive shuffles that occur when sorting of grouping by key, you may want more shuffle memory. Note that if either one starts to run low, your application won't crash it will simply spill to disk. This usually isn't as bad as it sounds especially if the OS file cache kicks in. \n",
    "\n",
    "You can check on the cache memory and shuffle memory in two ways while your application is running. In the Spark UI, you can see the cached RDDs under the `Storage` tab - if they start spilling to disk, this is where you will see it. Similarly, if you are running a large shuffle job, you can click on the stage details in the Spark UI and see the shuffle memory and disk statistics. We will check on both of these later on in this application. \n",
    "\n",
    "Here we will set these two options explicitly for completeness, but actually keep the values at their defaults (60%  of the heap for caching, 20% for shuffles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf.set('spark.storage.memoryFraction', 0.6)\n",
    "conf.set('spark.shuffle.memoryFraction', 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver memory\n",
    "\n",
    "The amount of memory allocated to the driver program could be crucial if the driver has to deal with a lot of late-stage aggegation products or if you want to collect a significant chunk of data out of the RDD. You can see how much memory has been allocated to the driver either in the Spark Web UI or in the messages printed to the console at initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf.set('spark.yarn.am.memory', '8g')\n",
    "conf.set('spark.yarn.am.cores', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PATH` and `PYTHONPATH`\n",
    "\n",
    "In some cases we need to tell the executors explicitly where the non-standard python libraries are located (this includes the spark libraries and seems to be new in Spark 1.4.0 -- a bug?). For this, we set the environment variable `PYTHONPATH`. Any other environment variable can be specified in this way, should it be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf.set('spark.executorEnv.PYTHONPATH', \n",
    "         '/cluster/apps/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip:/cluster/apps/spark/spark-1.4.0-bin-hadoop2.6/python/:/cluster/home03/sdid/roskarr/spark_workshop/gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a custom python (miniconda), we also set the `PATH` explicitly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf.set('spark.executorEnv.PATH', os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the `SparkContext`\n",
    "This is our entry point to the Spark runtime - it is used to push data into spark or load RDDs from disk etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'yarn-client', conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works successfully, you can check the [YARN application scheduler](http://a3000.hpc-net.ethz.ch:8088/cluster) and you should see your app listed there. Clicking on the \"Application Master\" link will bring up the familiar Spark Web UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a key-value RDD of book metadata and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data into spark from a collection of local files is a very common task. A useful pattern to keep in mind is the following: \n",
    "\n",
    "1. make a list of filenames and distribute it among the workers\n",
    "3. \"map\" each filename to the data you want to get out\n",
    "4. now you are left with the RDD of raw data distributed among the workers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case of the Gutenberg Project e-book data, we have a directory of `html` files which hold the actual book text, and another directory of associated metadata files (the RDF files). To make your life easier for the purpose of this tutorial, we have made a small python module called `gutenberg_cleanup` that has some handy functions for pulling out the relevant text and metadata out of the raw dataset. \n",
    "\n",
    "The [`gutenberg_cleanup`](gutenberg_cleanup.py) module contains three functions that can help with this: `get_gid`, `get_metadata` and `get_text`.\n",
    "\n",
    "They pretty much do the obvious: \n",
    "\n",
    "`get_gid` takes an html path and pulls out the book ID (`gid`)\n",
    "\n",
    "`get_metadata` takes a `gid` and returns a metadata object with various useful fields that will be used to create a unique key for each book\n",
    "\n",
    "`get_text` takes a path to an html file and returns the raw text extracted from HTML, cleaned of tags and punctuation and converted to lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the raw dataset using `sc.parallelize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a list of all html files in the data directory\n",
    "flist = glob.glob('/cluster/home03/sdid/roskarr/work/gutenberg/html/*html')\n",
    "print('number of books: ', len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `sc.parallelize` to distribute a dataset across the cluster, you can choose the number of partitions across which to distribute the dataset. The higher the number of partitions, the higher the \"parallelism\". When Spark subsequently executes maps and reduces on this dataset, it does so by dispatching tasks to different executors, which then request the cores under their control to do the actual work. By increasing the number of partitions, you increase the number of tasks - more tasks gives the Spark scheduler more flexibility in distributing the work across the cluster and therefore maximally leveraging the compute resources at its disposal. In some cases, where a single partition might require a lot of memory it can cause `Out of memory` errors - in such cases, simply reducing the amount of data per task by increasing the parallelism can help. \n",
    "\n",
    "Note that as long as the tasks take a few hundred milliseconds the scheduler should have no trouble dispatching them. On the other hand, there is a bit of overhead associated with partitioning the data so you don't want an unreasonably high number of partitions. You can see the [Spark guide](http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism) for a bit more detail. \n",
    "\n",
    "Below, we will choose to use 5 times as many partitions as we have cores in the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_rdd = sc.parallelize(flist, num_execs*exec_cores*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the list of filenames into a `key,value` pair RDD of metadata and text\n",
    "\n",
    "Use the `get_gid`, `get_text` and `get_metadata` functions to construct a key,value pair RDD, where `key` is the dictionary returned by `get_metadata`. For the `value` of each `key`,`value` pair use the raw text returned by `get_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gutenberg_cleanup\n",
    "from gutenberg_cleanup import get_metadata, get_text, get_gid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the doc strings for the three functions you'll need to ingest the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(get_gid)\n",
    "help(get_metadata)\n",
    "help(get_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `map` method of `files_rdd` to map the filenames to `(metadata, text)` tuples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd = (files_rdd.map(lambda filename: (get_metadata(get_gid(filename)), get_text(filename))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we don't have to constantly re-load the data off disk, lets cache this RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "text_rdd.cache()\n",
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "UI_url": {}
    }
   },
   "source": [
    "Since we called `count()`, it means that the entire RDD was generated/calculated. This combination of `cache` and `count` is a common way to check how much memory your dataset needs - once `count` completes you can check the memory taken up by the RDD by going to the \"Storage\" tag of the Spark UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is cached, next time you try to use `text_rdd` it will be much much quicker. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "count = text_rdd.count()\n",
    "assert(count == 15081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, we could call the native python `map` in exactly the same way (and run it on the local machine only), though this would take much longer to complete, i.e. \n",
    "\n",
    "    text = map(lambda f: (get_metadata(get_gid(f)), get_text(f)), flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at what this RDD looks like now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the metadata as a broadcast variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we manipulate the dataset, we will want ready access to the metadata, but we don't want to have to read it off the disk every time. We can extract it from the RDD and save it in a dictionary for later use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the list of fields\n",
    "metadata_fields = text_rdd.first()[0].keys() # this takes the metadata of the first element and extracts the keys\n",
    "meta_dict = dict()\n",
    "for meta in text_rdd.keys().collect() :\n",
    "    meta_dict[meta['gid']] = {key: meta[key] for key in metadata_fields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a look-up table that allows us to quickly access all the metadata indexed by `gid`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_dict[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get, for example, the author birth year for book with `gid = 101`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_dict[101]['birth_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need this lookup table often in the analysis. Since it will be used in many `map` transformations, we would have to send it across the wire every time we used it. Alternatively, we could read the metadata off the disk every time, but this is even worse for many reasons (for one, there are many thousands of RDF files). \n",
    "\n",
    "Instead, Spark offers us a [broadcast variable](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) mechanism. This allows us to distribute a non-trivial piece of data to all nodes and keep it there. When the code running on the nodes needs a value from the broadcast variable, it is simply grabbed from memory. Since the variable is stored in the JVM on the executor, if the executor runs many cores, those cores can share the data therefore even further reducing unnecessary network traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call it meta_b for 'broadcast'\n",
    "meta_b = sc.broadcast(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying data object stored in `meta_b` can be accessed simply by\n",
    "\n",
    "    > meta_b.value\n",
    "    \n",
    "We'll make use of this soon. If you check the console output, you will see an INFO message that the broadcast has been created, i.e. \n",
    "\n",
    "```\n",
    "15/06/24 17:18:44 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 910.7 KB, free 4.1 GB)\n",
    "15/06/24 17:18:44 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.201.20.22:47821 (size: 910.7 KB, free: 4.1 GB)\n",
    "15/06/24 17:18:44 INFO spark.SparkContext: Created broadcast 6 from broadcast at PythonRDD.scala:403\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw dataset to HDFS (or local storage)\n",
    "\n",
    "As a final bit of preparation before continuing with analysis, we save the raw data in a way that makes it faster to access later. We don't want to have to read the data off local disk every time we need to repeat some part of the analysis. Instead, it's much more advantageous to use the Hadoop Distributed File System (HDFS) to store the data once we've read it in and put it in a `key,value` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd.saveAsPickleFile('hdfs:///user/<YOUR_USERNAME>/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, whenever we need it, we can read the data off the HDFS instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_text_rdd = sc.pickleFile('hdfs:///user/<YOUR_USERNAME>/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time loaded_text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this read time to nearly two minutes it took to read it off the local filesystem initially. If you look at the details for this stage in the Spark UI you can understand why this is: in the column named \"locality level\", you see that for many tasks it says `NODE LOCAL` while for others it might say `RACK LOCAL`. These mean that either the data chunk was physically present on the disk of the node that was reading it in (`NODE LOCAL`) or it was on one of the nodes on the same switch (`RACK LOCAL`). Of course the additional advantage is not having to deal with the filesystem overhead of 10k+ small files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data with filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to do some quality checks on the data. Let's check out the first couple of metadata entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at just the first few entries it becomes clear that we're going to have to do some quality control here. For example, we probably don't want books with \"None\" as either of the author names, and likewise we have to have the birth date in order to be able to create a time series out of the data in the end. \n",
    "\n",
    "Construct an RDD, as above, except that you filter out all the elements that have `None` for `title`, `first_name`, `last_name`, or `birth_year`. In addition, filter out the data with \"BC\" in either birth or death year. \n",
    "\n",
    "The `filter_func` has already been defined for you, but you need to apply it to `text_rdd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_func(meta) : \n",
    "    no_none = all([meta[name] is not None for name in ['title', 'first_name', 'last_name', 'birth_year']])\n",
    "    if not no_none : \n",
    "        return False\n",
    "    else : \n",
    "        no_birth_bc = 'BC' not in meta['birth_year']\n",
    "        no_death_bc = True if meta['death_year'] is None else 'BC' not in meta['death_year']\n",
    "        return no_birth_bc + no_death_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_rdd = text_rdd.filter(lambda (meta, text): filter_func(meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have left? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = filtered_rdd.count()\n",
    "print('number of books after filtering: ', count)\n",
    "assert(count == 11872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final bit of cleanup: \n",
    "\n",
    "some of the books end up in multiple files, but they should all have the same `gid`. \n",
    "\n",
    "To check for this we will use one of the most basic and common MapReduce patterns -- the key count: \n",
    "\n",
    "* map the data into `key`,`value` pairs where `key` is the quantity we want to count and `value` is just 1. \n",
    "* invoke a reduction *by key*, where the reduction operator is a simple addition\n",
    "\n",
    "Finally, we will sort the result and print out the first few elements to check whether we have to worry about documents spanning multiple files or not. \n",
    "\n",
    "The RDD operations that are needed are [`reduceByKey`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) and [sortBy](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy).\n",
    "\n",
    "For the `keyFunc` of the call to `sortBy`, use a `lambda` function that extracts the counts obtained from the `reduceByKey`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_filtered = filtered_rdd.map(<FILL>)\n",
    "\n",
    "reduced_map = filter_map.reduceByKey(add)\n",
    "\n",
    "sorted_reduced = reduced_map.sortBy(<FILL>)\n",
    "\n",
    "sorted_reduced.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are several transformations here that lead to the final result, `sorted_reduced`. A common syntax is to group them all together, by enclosing them in `( )` and chaining them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "(filtered_rdd.map(<FILL>)\n",
    "             .reduceByKey(add)\n",
    "             .sortBy(<FILL>)\n",
    "             .take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a few that are made up of multiple sections. To combine them, we will use `reduceByKey` which will result in having an RDD of `gid`'s as keys and the combined text of each `gid`. The reduction function in `reduceByKey` can be a simple in-line function that just adds two elements together (but can't be the `add` function because that expects the arguments to be numbers). \n",
    "\n",
    "1. `map` `filtered_rdd` to (gid, text) tuples\n",
    "2. `reduceByKey` adding together the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "cleaned_rdd = (filtered_rdd.map(<FILL>)\n",
    "                           .reduceByKey(<FILL>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple sanity check, lets look at `gid`=6478, which according to the cell above has 43 sections in the original dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(filtered_rdd.map(lambda (meta, text): (meta['gid'],1))\n",
    "                .lookup(6478))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cleaned_rdd.lookup(6478))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to do all these pre-processing steps again at a later point, lets also save the `cleaned_rdd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_rdd.saveAsPickleFile('/user/<YOUR_USERNAME>/gutenberg/cleaned_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now saved in the directory we specified, one file per partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /user/roskarr/gutenberg/cleaned_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're finished with the basic pre-processing. Our `cleaned_rdd` contains `gid`'s as keys and text as values. If we want some other piece of metadata, we can just access it via the lookup table, for example `meta_b.value[gid][meta_name]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of book publication years\n",
    "Now we're ready to start asking some questions of the data. To begin with, lets do a simple histogram of the year distribution of the books. Since we don't have original publication dates, we just use the simple formula: \n",
    "\n",
    "$year = max\\left((year_{birth} + year_{death})/2, year_{birth} + offset\\right)$, \n",
    "\n",
    "where $offset$ is a number drawn from a gaussian centered on 40 with $\\sigma = 5$ years. This means that we assume most people write their books around 40. ;)\n",
    "\n",
    "The function `publication_year` is provided for you and you should use it to *transform* the `cleaned_rdd` into an RDD of publication years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def publication_year(meta) : \n",
    "    birth_year = int(meta['birth_year'])\n",
    "    if meta['death_year'] is None : \n",
    "        year = birth_year + np.random.normal(40,5)\n",
    "    else :\n",
    "        death_year = int(meta['death_year'])\n",
    "        year = max((birth_year + death_year) / 2.0, birth_year+np.random.normal(40,5))\n",
    "\n",
    "    return min(int(year),2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "year_rdd = (cleaned_rdd.map(<FILL>)\n",
    "                       .cache())\n",
    "year_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram function actually already exists in the Spark API (but it didn't use to!). However, for fun we will write our own. Calculating the histogram can be split up into two parts. First, we need to figure out which bin each value corresponds to: \n",
    "\n",
    "1. take bins and a value as input\n",
    "2. calculate the bin that the value maps to and return (`bin`, 1) pair\n",
    "\n",
    "Second, we need to do a simple `reduceByKey` where we just add up all the values belonging to each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a helper function to determine which bin a value falls into\n",
    "from bisect import bisect_right\n",
    "def get_bin(bin_edges, value) : \n",
    "    \"\"\"Returns which bin, specified by `bin_edges` the `value` falls into.\"\"\"\n",
    "    return bisect_right(bin_edges, value) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram(rdd, nbins = 100, min_val=None, max_val=None) :\n",
    "    # if either min_val or max_val are missing, get them from the data\n",
    "    if min_val is None : \n",
    "        min_val = rdd.<FILL>\n",
    "    if max_val is None : \n",
    "        max_val = rdd.<FILL>\n",
    "        \n",
    "    bin_edges = np.linspace(min_val,max_val,nbins+1)\n",
    "    \n",
    "    binned_rdd = rdd.map(<FILL>)\n",
    "    \n",
    "    summed_bins = binned_rdd.reduceByKey(<FILL>).collect() # FILL IN \n",
    "    \n",
    "    # This is a sparse result -- turn into a dense vector for plotting: \n",
    "    res_full = np.zeros(nbins)\n",
    "    overflow = 0\n",
    "    for item in summed_bins : \n",
    "        if item[0] > len(res_full)-1 or item[0] < 0: \n",
    "            continue # ignore\n",
    "        else: res_full[item[0]] = item[1]\n",
    "            \n",
    "    return .5*(bin_edges[:-1]+bin_edges[1:]), res_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time bins, vals = histogram(year_rdd, min_val=1500, max_val=2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of operation is a good candidate for computing a partial result on each partition followed by a reduce step. This can dramatically reduce the network traffic. Instead of mapping each value to a bin and doing a potentially expensive shuffle during the `reduceByKey` operation, we can compute a histogram locally on each partition and then simply add them up at the end. \n",
    "\n",
    "Here is where our knowledge of generators comes in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_partition(iterator, nbins,  min_val, max_val) : \n",
    "    from bisect import bisect_right\n",
    "    \n",
    "    bin_edges = np.linspace(min_val,max_val,nbins+1)\n",
    "    \n",
    "    bin_vals = np.zeros(len(bin_edges)-1)\n",
    "    \n",
    "    for item in iterator : \n",
    "        try : \n",
    "            bin_vals[bisect_right(bin_edges, item)-1] += 1\n",
    "        except IndexError : \n",
    "            pass\n",
    "        \n",
    "    yield bin_vals\n",
    "\n",
    "def histogram_partition(rdd, nbins = 100, min_val=None, max_val=None) :\n",
    "    # if either min_val or max_val are missing, get them from the data\n",
    "    if min_val is None : \n",
    "        min_val = rdd.min() \n",
    "    if max_val is None : \n",
    "        max_val = rdd.max() \n",
    "        \n",
    "    bin_edges = np.linspace(min_val,max_val,nbins+1)\n",
    "    \n",
    "    result = (rdd.mapPartitions(lambda iterator: bin_partition(iterator, nbins, min_val, max_val))\n",
    "                 .reduce(lambda a,b: a+b))\n",
    "    \n",
    "    return .5*(bin_edges[:-1]+bin_edges[1:]), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time bins, vals = histogram_partition(year_rdd, min_val=1500, max_val=2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here doesn't look dramatic because the total amount of data is rather small, but have a look at the Spark Web UI and you will see that the second implementation didn't do any shuffle writing. With a bigger data set, this difference would potentially matter a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(bins, vals, color = '#3182bd', lw=2)\n",
    "plt.xlim(1500,2015)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('Number of books')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the metadata some more\n",
    "\n",
    "Lets do a couple more checks and practice using the Spark API. \n",
    "\n",
    "How many unique authors are there in the dataset? \n",
    "\n",
    "1. make `author_rdd` that is composed of a string `\"last_name, first_name\"`\n",
    "2. keep only the unique author strings (*hint*: look at the Spark API to find an appropriate method)\n",
    "3. count the number of elements remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "author_rdd = cleaned_rdd.map(<FILL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "n_authors = (author_rdd.<FILL>\n",
    "                       .count())\n",
    "print(\"Number of distinct authors: %s \" % n_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most-represented authors in the corpus: \n",
    "\n",
    "1. use the `author_rdd` from above\n",
    "2. use the pattern `(key, 1)` to set up an RDD that can be passed to `reduceByKey`\n",
    "3. run `reduceByKey`, yielding an RDD composed of `(author, count)` tuples\n",
    "4. sort by descending order of number of books per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "(author_rdd.map(<FILL>)\n",
    "           .reduceByKey(<FILL>)\n",
    "           <FILL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets do the same thing per language, just to get an idea of how much data there is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "lang_rdd = cleaned_rdd.map(<FILL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN: reduce the `lang_rdd` to yield number of books in each language\n",
    "lang_rdd.<FILL>.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many words were used in these 500+ years? \n",
    "### Construct a corpus-wide vocabulary\n",
    "\n",
    "We could have done the above metadata gymnastics without ever invoking a distributed processing framework of course by simply extracting the years from the metadata. The text body of each data element is where the bulk of the data volume lies. \n",
    "\n",
    "To construct a corpus wide vocabulary, we have to deconstruct each document into a list of words and then extract the unique words from the entire data set. If our dataset fit into memory of a single machine, this is a simple set operation. But what if it doesn't? \n",
    "\n",
    "We'll assume this is the case and instead of converting each `gid,text` pair into a `gid,list_of_words` pair, we will simply construct one RDD of words. Here we aren't necessarily interested in preserving the provenance of words, but just finding the unique words in the whole corpus. \n",
    "\n",
    "1. map the entire RDD of text into an RDD of single words\n",
    "2. use the `distinct` method of the resulting RDD to transform it into an RDD with only unique words\n",
    "\n",
    "*Hint:* In python, splitting a string into a set of words separated by spaces is easy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = 'splitting a string is super simple'\n",
    "line.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an RDD `distinct_rdd` which holds the *unique English* words. Consider the steps this will require:\n",
    "\n",
    "* use `cleaned_rdd` but keep only books in the english language\n",
    "* convert each document into individual words\n",
    "* retain only the unique words\n",
    "\n",
    "Which RDD methods can you use to achieve these three steps? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN\n",
    "distinct_rdd = (cleaned_rdd.filter(<FILL>)\n",
    "                           .<FILL>\n",
    "                           .<FILL>)\n",
    "print(\"Number of unique words: \", distinct_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question: can you write this by using `mapPartitions`? It's much faster...\n",
    "\n",
    "*hint*: use the python [set](https://docs.python.org/2/library/stdtypes.html#set) to find unique words in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FILL IN -- remember that partition_set has to be a generator that yields unique words from a partition\n",
    "def partition_set(iterator): \n",
    "    words = set()\n",
    "    <FILL> \n",
    "        \n",
    "res = (cleaned_rdd.filter(lambda (gid, text): meta_b.value[gid]['lang'] == 'en')\n",
    "                  .mapPartitions(partition_set)\n",
    "                  .distinct())\n",
    "res.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"MapReduce\" tutorial would self destruct if it didn't include a word counting example. So, lets count the occurences of all the words across the entire corpus. This is a fairly straightforward operation: \n",
    "\n",
    "0. keep only the english language books\n",
    "1. `map` each word into a (`word, count`) pair\n",
    "2. call `reduceByKey` to sum up all the `count`s for each word\n",
    "3. finally to make it useful, sort it in descending order\n",
    "\n",
    "*hint:* think about which mapping method you need to use to convert the single item (`text`) into many items (individual words)\n",
    "\n",
    "*hint \\#2:* list comprehension can lead to a nice concise solution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count = (cleaned_rdd.filter(<FILL>)\n",
    "                         .<FILL>\n",
    "                         .reduceByKey(lambda a,b: a+b)\n",
    "                         .<FILL>\n",
    "                         .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduces, Shuffles, and Partitioning\n",
    "During a `reduceByKey`, or any other reduction for that matter, data must be shuffled around the cluster and combined. By default, this is done in an intelligent way by first reducing values locally on each partition, and then combining the results of the partitions. Still, as is the case here, for common keys, every partition will have to send its results to others. This can result in a lot of temporary file IO if the data that needs to be communicated can't all be held in memory on all of the executors. \n",
    "\n",
    "One way around this is to partition the data ahead of time so that the same keys land on the same partition by design. This results in much less data needing to be shipped around the network and can improve the performance. Of course, at the cost of an expensive initial shuffle! But if many reductions have to be done on the same data, it might be worth it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "stop_words = load(open('./stop_words.dump'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_rdd = (cleaned_rdd.filter(lambda (gid, text): meta_b.value[gid]['lang'] == 'en')\n",
    "                       .flatMap(lambda (gid, text): [(word,1) for word in text.split() if word not in stop_words])\n",
    "                       .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we re-partition the `word_rdd` using the built-in hashing function, which just turns the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_partitions = word_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "par = word_rdd.partitionBy(num_partitions, lambda x: hash(x)%num_partitions).cache()\n",
    "par.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both datasets are cached in memory, we can compare the time it takes the reduce step to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "word_rdd.reduceByKey(lambda a,b: a+b).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "par.reduceByKey(lambda a,b: a+b).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A ~ 40% improvement! Not bad, though we can expect the difference to depend on the nature of the dataset. If you inspect the Spark UI, you can see that the first `reduceByKey` (i.e. one done on `word_rdd`) shuffled ~390 Mb of data, while the second `reduceByKey` (i.e. done on `par`) only shuffled ~50 Mb of data. This dataset is still pretty small, but when the shuffles are in the Gb range, the differences can be substantial. \n",
    "\n",
    "If you need to do lookups of individual keys, this becomes even more dramatic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time x = par.lookup('environment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time x = word_rdd.lookup('environment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Vectors\n",
    "### Computing word frequency vs. time\n",
    "\n",
    "Here we'll build a simple tool that visualizes the relative word frequency as a function of time in the Gutenberg corpus. For inspiration, see the [Google Ngram viewer](https://books.google.com/ngrams).\n",
    "\n",
    "To do this, we will convert each document into a vector that represents the word counts for each word appearing in the document. In order for the vector indices to remain consistent across the whole corpus, we have to build a corpus-wide lookup table, a `word --> index` mapping. \n",
    "\n",
    "Then, once each document is converted to a vector, we will reduce the vectors by year, yielding an RDD that will have the total number of occurrences of each word in every year. From there, it is trivial to look up the desired word and plot the relative frequency vs. year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the lookup table\n",
    "Create a look-up table of words by attaching a unique index to each word. The Spark API provides a [zipWithIndex](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) method that makes this easy. \n",
    "\n",
    "Follow these steps: \n",
    "\n",
    "1. use the sorted `word_count` RDD we created above to begin\n",
    "2. use `zipWithIndex` to give each word a unique identifier\n",
    "2. filter the resulting RDD to include only the first 100000 words (they are sorted by count, so this will be the 100000 most common ones) \n",
    "3. Finally, use [collectAsMap](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap) to return the resulting RDD to the driver as a dictionary. \n",
    "\n",
    "Make sure you understand what the result of `zipWithIndex` is -- your result should be a dictionary using words as keys and indices as values and nothing else!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "word_lookup = (word_count.<FILL>\n",
    "                         .<FILL>\n",
    "                         .<FILL>\n",
    "                         .collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast `word_lookup` so we can use it on all the workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_lookup_b = sc.broadcast(word_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "def split_iter(string):\n",
    "    \"\"\"\n",
    "    Generator replacement for string.split() to use minimal memory \n",
    "    overhead when iterating over an entire document\n",
    "    \"\"\"\n",
    "    return (x.group(0) for x in re.finditer(r\"[A-Za-z']+\", string))\n",
    "\n",
    "def count_doc_words(doc, vocab) : \n",
    "    \"\"\"\n",
    "    Returns a vector representation of `doc` given the reference \n",
    "    vocabulary `vocab`. \n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    from scipy.sparse import csr_matrix \n",
    "    \n",
    "    d = defaultdict(int)\n",
    "    \n",
    "    for word in split_iter(doc) : \n",
    "        if word in vocab : \n",
    "            d[word] += 1\n",
    "        \n",
    "    values = np.empty(len(d))\n",
    "    indices = np.empty(len(d))\n",
    "    \n",
    "    for i, (word, val) in enumerate(d.iteritems()) : \n",
    "        indices[i] = vocab[word]\n",
    "        values[i] = val\n",
    "        \n",
    "    return csr_matrix((values, (indices, np.zeros(len(d)))), shape = (len(vocab), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD of english-language books vectorized using the most common 100k words and keyed by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FILL IN \n",
    "eng_rdd = cleaned_rdd.<FILL>\n",
    "\n",
    "# make the key by year\n",
    "year_rdd = eng_rdd.map(lambda (gid, text): <FILL>)\n",
    "\n",
    "# use the function defined above to vectorize the document\n",
    "vector_year_rdd = year_rdd.map(lambda (year, text) : <FILL>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector_year_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the entire Gutenberg English book corpus in the form of sparse vectors encoding the most used 100k words. \n",
    "\n",
    "Getting yearly sums is now a simple `reduceByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "year_sums = vector_year_rdd.reduceByKey(lambda a,b: a+b)\n",
    "year_data = year_sums.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_usage_frequency(words, year_data, word_lookup, plot_range = [1500,2015]) : \n",
    "    years = sorted(year_data.keys())\n",
    "    tot_count = np.array([year_data[year].sum() for year in years])\n",
    "    \n",
    "    if type(words) is not str : \n",
    "        n_words = len(words) \n",
    "    else : \n",
    "        n_words = 1\n",
    "        words = [words]\n",
    "        \n",
    "    for i, word in enumerate(words) : \n",
    "        word_ind = word_lookup[word]\n",
    "        w_count = np.array([year_data[year][word_ind].toarray().squeeze() for year in years])\n",
    "        \n",
    "        plt.plot(years, smooth(w_count/(tot_count-w_count)),label=word, color = plt.cm.Set1(1.*i/n_words))\n",
    "    \n",
    "    plt.xlim(plot_range)\n",
    "    plt.xlabel('year')\n",
    "    plt.ylabel('relative frequency')\n",
    "    plt.legend(loc='upper left', fontsize = 'small')\n",
    "    \n",
    "    \n",
    "def smooth(x,window_len=11,window='hanning'):\n",
    "        if x.ndim != 1:\n",
    "                raise ValueError, \"smooth only accepts 1 dimension arrays.\"\n",
    "        if x.size < window_len:\n",
    "                raise ValueError, \"Input vector needs to be bigger than window size.\"\n",
    "        if window_len<3:\n",
    "                return x\n",
    "        if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "                raise ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "        s=np.r_[2*x[0]-x[window_len-1::-1],x,2*x[-1]-x[-1:-window_len:-1]]\n",
    "        if window == 'flat': #moving average\n",
    "                w=np.ones(window_len,'d')\n",
    "        else:  \n",
    "                w=eval('np.'+window+'(window_len)')\n",
    "        y=np.convolve(w/w.sum(),s,mode='same')\n",
    "        return y[window_len:-window_len+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some fun ones: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_usage_frequency(['giveth', 'environment', 'machine'], year_data, word_lookup, [1300, 2015])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
