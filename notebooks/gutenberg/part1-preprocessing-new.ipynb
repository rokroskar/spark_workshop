{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:spark-workshop:No hadoop configuration found; is HADOOP_CONF_DIR set? Proceeding in Spark standalone mode\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('spark-workshop')\n",
    "\n",
    "from IPython.display import HTML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "try:\n",
    "    tree = ET.parse(os.environ['HADOOP_CONF_DIR'] + '/yarn-site.xml')\n",
    "    root = tree.getroot()\n",
    "    yarn_web_app = root.findall(\"./property[name='yarn.resourcemanager.webapp.address']\")[0].find('value').text\n",
    "    yarn_web_app_string = \"\"\"\n",
    "    If this works successfully, you can check the \n",
    "    <a target='_blank' href='http://{yarn_web_app}'>YARN application scheduler</a> and you should see \n",
    "    your app listed there. \n",
    "    Clicking on the 'Application Master' link will bring up the familiar Spark Web UI.\n",
    "    \"\"\"\n",
    "    USE_HADOOP = True\n",
    "except (IOError, KeyError) as e:\n",
    "    USE_HADOOP = False\n",
    "    logger.info(\"No hadoop configuration found; is HADOOP_CONF_DIR set? Proceeding in Spark standalone mode\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of this tutorial\n",
    "\n",
    "#### In this notebook, we will pre-process the text corpus using Spark\n",
    "\n",
    "The basic steps will be something like:\n",
    "\n",
    "2. create a metadata lookup table that can be distributed to the workers\n",
    "3. clean the dataset for missing values and repeated entries\n",
    "4. save the final, cleaned dataset to disk\n",
    "\n",
    "To achieve this, you will learn many concepts along the way, including:\n",
    "\n",
    "* basic Spark setup and initializing a `SparkContext` on a cluster\n",
    "* loading raw data into a Spark Resilient Distributed Dataset (RDD)\n",
    "* using basic RDD manipulations (`map`, `filter`, `first`, `take`) for data inspection\n",
    "* the usefulness of broadcast variables\n",
    "* reduction operations (e.g. `reduceByKey`)\n",
    "\n",
    "Lets get started with setting up the Spark runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the spark environment can be a bit of a trial and error procedure. Often you'll need to configure settings (in particular dealing with memory) to fit your cluster and your particular application. Below, we will specify a few of the most important ones -- but you can see the full list in the [Spark Configuration guide](http://spark.apache.org/docs/latest/configuration.html) and if you are using YARN there are critical options also listed under the [YARN deployment guide](http://spark.apache.org/docs/latest/running-on-yarn.html).\n",
    "\n",
    "This is also a great time to have a look at the [Transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations) and [Actions](http://spark.apache.org/docs/latest/programming-guide.html#actions) in the Spark programming guide or the list of [RDD methods in the Spark API](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python environment setup\n",
    "\n",
    "First, we need to make sure that the `pyspark` libraries are accessible in this notebook. To do this, we can add them to the library search path with a convenient `findspark` library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = os.path.join(os.environ['HOME'], 'spark')\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime settings\n",
    "\n",
    "Below we will specify a range of settings for the Spark runtime system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><strong>Note</strong></p> \n",
    "\n",
    "<p>The environment variables have to be declared before any other spark initialization takes place (including creating a <code>SparkConf</code> object.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncores = int(os.environ.get('LSB_DJOB_NUMPROC', 1)) # how many cores do we have for the driver\n",
    "\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '%dG'%(ncores*2*0.7)\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing `SparkConf` \n",
    "\n",
    "When starting the Spark runtime through the notebook or inside a script (i.e. when not calling one of the spark scripts like `spark_submit`), you can create a `SparkConf` object that allows you to set up the runtime. This is quite convenient and much cleaner and more readable than specifying the options on the commandline. \n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin-top: 10px\">\n",
    "<p><strong>Pro tip</strong></p>\n",
    "\n",
    "<p>If you use the same base configuration often, you can store it in a file and only override those that are needed through `SparkConf` (we'll do this at the end of this section).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initializing the SparkConf\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor options\n",
    "\n",
    "The full list of options is very long, but the basic ones you'll *always* want to at least think about are ones pertaining to the basic configuration of the executors: number of executors, memory per executor, and number of cores per executor. \n",
    "\n",
    "A few notes about the memory configuration: the `spark.executor.memory` should not be set to the total memory of the node. Some memory is needed for the OS (including HDFS and other services), and still more is required for the Spark overhead. So in our case here, we have 16 Gb of memory per node but can only use around 12 Gb of this for the executors. Since we need to leave room for 10% YARN overhead, we specify 9 Gb here to be safe. If your executors start dying off for strange reasons, try reducing the memory here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_executors = 5\n",
    "cores_per_executor = 4\n",
    "memory_per_core = 2000\n",
    "\n",
    "# these options in SparkConf are only used if running the job with Hadoop\n",
    "if USE_HADOOP: \n",
    "    conf.set('spark.executor.memory', '9g')\n",
    "    conf.set('spark.executor.instances', str(num_execs))\n",
    "    conf.set('spark.executor.cores', str(exec_cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory configuration\n",
    "\n",
    "Two other key memory options exist, specifying the amount of executor memory reserved for *cached* data and for *shuffle* data. Depending on what your application is doing, you may need more of one or the other. For example, if you are running a lot of iterative operations on a large dataset, you probably want a good amount of memory for RDD caching. On the other hand, if you are doing lots of expensive shuffles that occur when sorting of grouping by key, you may want more shuffle memory. Note that if either one starts to run low, your application (hopefully) won't crash, but instead simply spill to local disk. \n",
    "\n",
    "You can check on the cache memory and shuffle memory in two ways while your application is running. In the Spark UI, you can see the cached RDDs under the `Storage` tab - if they start spilling to disk, this is where you will see it. Similarly, if you are running a large shuffle job, you can click on the stage details in the Spark UI and see the shuffle memory and disk statistics. We will check on both of these later on in this application. \n",
    "\n",
    "Here we will set these two options explicitly (30% of the heap for caching, 50% for shuffles). Note that the default here is 60% for caching and 20% for shuffles, but we expect to have some fairly memory-hungry shuffles in our application, so we adjust the allocations a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### THESE ARE USUALLY NO LONGER NEEDED IN SPARK > 1.6\n",
    "# conf.set('spark.storage.memoryFraction', 0.3)\n",
    "# conf.set('spark.shuffle.memoryFraction', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver memory\n",
    "\n",
    "The amount of memory allocated to the driver program could be crucial if the driver has to deal with a lot of late-stage aggegation products or if you want to collect a significant chunk of data out of the RDD. You can see how much memory has been allocated to the driver either in the Spark Web UI or in the messages printed to the console at initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if USE_HADOOP:\n",
    "    conf.set('spark.yarn.am.memory', '8g')\n",
    "    conf.set('spark.yarn.am.cores', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a saved configuration\n",
    "\n",
    "Often, you will probably have a few different jobs that all need the same configuration. Instead of always specifying it inside the notebook, it might be a little cleaner to save it in an external file. \n",
    "\n",
    "In this case, we have the configuration saved in `~/spark_workshop/notebooks/gutenberg/spark_config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Some default options for Spark \r\n",
      "\r\n",
      "spark.executor.memory 8g\r\n",
      "spark.executor.instances 20\r\n",
      "spark.executor.cores 4\r\n",
      "spark.storage.memoryFraction 0.5\r\n",
      "spark.shuffle.memoryFraction 0.2\r\n",
      "spark.yarn.executor.memoryOverhead 2048\r\n",
      "spark.yarn.am.memory  8g\r\n",
      "spark.yarn.am.cores  4\r\n",
      "spark.master yarn\r\n",
      "spark.app.name gutenberg"
     ]
    }
   ],
   "source": [
    "!cat ./spark_config/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time we start the `SparkContext` we can simply set the `SPARK_CONF_DIR` environment variable to point to the directory where our configuration lives, and it will be taken care of automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><strong>Pro tip</strong></p> \n",
    "\n",
    "<p>You can edit the configuration straight from Jupyter by navigating to it directly in the Jupyter file browser.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the standalone spark cluster\n",
    "\n",
    "If we are not using hadoop, we have to request resources from the cluster scheduler. We use the [`sparkhpc`]('http://sparkhpc.readthedocs.io') package to simplify the deployment and management of the standalone cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sparkhpc import sparkjob\n",
    "sj = sparkjob.sparkjob(ncores=number_of_executors*cores_per_executor, \n",
    "                       cores_per_executor=cores_per_executor,\n",
    "                       memory_per_core=memory_per_core,\n",
    "                       walltime='1:00')\n",
    "                      # extra_scheduler_options='#BSUB -R beta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Job <42905513> is submitted to queue <normal.4h>.\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "sj.wait_to_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sparkhpc import sparkjob\n",
    "sj = sparkjob.sparkjob(clusterid=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the `SparkContext`\n",
    "\n",
    "This is our entry point to the Spark runtime - it is used to push data into spark or load RDDs from disk etc. Note that here we specify `master = 'yarn-client'`, which will automatically request the resources we have configured in `conf` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if USE_HADOOP:\n",
    "    sc = SparkContext(master = 'yarn-client', conf = conf)\n",
    "else:\n",
    "    time.sleep(5)\n",
    "    sc = sj.start_spark()    \n",
    "sc.addPyFile('./gutenberg_cleanup.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<tr><td>ClusterID</td>\n",
       "                    <th>Job ID</th>\n",
       "                    <th>Number of cores</th>\n",
       "                    <th>Status</th>\n",
       "                    <th>Spark UI</th>\n",
       "                    <th>Spark URL</th>\n",
       "                    </tr><tr><td>0</td>\n",
       "                    <td>42905513</td>\n",
       "                    <td>20</td>\n",
       "                    <td>running</td>\n",
       "                    <td><a target=\"_blank\" href=\"http://10.205.18.48:8080\">http://10.205.18.48:8080</a></td>\n",
       "                    <td>spark://10.205.18.48:7077</td>\n",
       "                  </tr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_HADOOP:\n",
    "    HTML(yarn_web_app_string.format(yarn_web_app=yarn_web_app))\n",
    "else: \n",
    "    sj.show_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset from HDFS\n",
    "\n",
    "I have already pre-loaded the data for you to avoid some tedious boilerplate code. If you want to see the process of loading the data into Spark from local disk anyway, you can have a look at the [data input notebook](part0-data-input.ipynb).\n",
    "\n",
    "Here, we just load the data from my HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_rdd_path = '/cluster/home/roskarr/projects/gutenberg_data/rdds/raw_text_rdd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd = sc.pickleFile(raw_text_rdd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 20 ms, total: 76 ms\n",
      "Wall time: 41.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25379"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time text_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation loaded the data either from the Hadoop distributed filesystem (HDFS) or a local filesystem and created an RDD. If you look at the details for this stage in the Spark UI (click on the \"Stages\" tab at the top, then click on the \"count\" stage at the top of the list) you can understand why this is: in the column named \"locality level\", you see that for many tasks it says `NODE LOCAL` while for others it might say `RACK LOCAL`. These mean that either the data chunk was physically present on the disk of the node that was reading it in (`NODE LOCAL`) or it was on one of the nodes on the same switch (`RACK LOCAL`). Of course the additional advantage is not having to deal with the filesystem overhead of 40k+ small files. \n",
    "\n",
    "You can also check how much memory this raw data occupies by clicking on the \"Storage\" tab in the Spark UI. Note that if the value reported there says the data is serialized, it means it's also likely compressed so the actual size is a bit larger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at what this RDD looks like -- this is often most easily done using the `first` method of the RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata:  {'birth_year': None, 'subtitle': '', 'license': 'Public domain in the USA.', 'death_year': None, 'title': \"McClure's Magazine, Vol. 6, No. 2, January, 1896\", 'last_name': 'Various', 'gid': '13637', 'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/3/13637/13637-8.txt', 'language': 'en', 'file_types': {'13637-h.zip': 'text/html; charset=iso-8859-1', '13637.txt.utf-8': 'text/plain', '13637-8.txt': 'text/plain; charset=iso-8859-1', '13637.epub.images': 'application/epub+zip', '13637-h.htm': 'text/html; charset=iso-8859-1', '13637.txt': 'text/plain; charset=us-ascii', '13637.kindle.noimages': 'application/x-mobipocket-ebook', '13637.rdf': 'application/rdf+xml', '13637.kindle.images': 'application/x-mobipocket-ebook', '13637.epub.noimages': 'application/epub+zip'}, 'downloads': '13', 'author_name': ['Various'], 'author_id': '116', 'first_name': None}\n",
      "\n",
      "text:  The Project Gutenberg EBook of McClure's Magazine, January, 1896, Vol. VI.\n",
      "No. 2, by Various\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "\n",
      "Title: McClure's Magazine, January, 1896, Vol. VI. No. 2\n",
      "\n",
      "Author: Various\n",
      "\n",
      "Release Date: October 5, 2004 [EBook #13637]\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: ISO-8859-1\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK MCCLURE'S MAGAZINE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Juliet Sutherland, Richard J. Shiffer and the PG Online\n",
      "Distributed Proofreading Team\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Note: The Table of Contents and the list of illustrations were added\n",
      "  by the transcriber.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  McCLURE'S MAGAZINE\n",
      "\n",
      "  JANUARY, 1896\n",
      "\n",
      "  Vol. VI, JANUARY, 1896, NO. 2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  TABLE OF CONTENTS\n",
      "\n",
      "  ILLUSTRATIONS\n",
      "  ABRAHAM LINCOLN. Edited by Ida M. Tarbell.\n",
      "    Lincoln's First Experiences in Illinois.\n",
      "    In C\n"
     ]
    }
   ],
   "source": [
    "# TODO: use the 'first()' method of the text_rdd to extract the the meta and text out of the first element \n",
    "meta, text = text_rdd.first()\n",
    "print('metadata: ', meta)\n",
    "print('\\ntext: ', text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data element we have a tuple containing a dictionary of metadata (`'language'`, `'first_name'` etc.), and the text. Lets have another look at what's inside the metadata dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birth_year:  None\n",
      "subtitle:  \n",
      "license:  Public domain in the USA.\n",
      "death_year:  None\n",
      "title:  McClure's Magazine, Vol. 6, No. 2, January, 1896\n",
      "last_name:  Various\n",
      "gid:  13637\n",
      "filename:  /cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/3/13637/13637-8.txt\n",
      "language:  en\n",
      "file_types:  {'13637-h.zip': 'text/html; charset=iso-8859-1', '13637.txt.utf-8': 'text/plain', '13637-8.txt': 'text/plain; charset=iso-8859-1', '13637.epub.images': 'application/epub+zip', '13637-h.htm': 'text/html; charset=iso-8859-1', '13637.txt': 'text/plain; charset=us-ascii', '13637.kindle.noimages': 'application/x-mobipocket-ebook', '13637.rdf': 'application/rdf+xml', '13637.kindle.images': 'application/x-mobipocket-ebook', '13637.epub.noimages': 'application/epub+zip'}\n",
      "downloads:  13\n",
      "author_name:  ['Various']\n",
      "author_id:  116\n",
      "first_name:  None\n"
     ]
    }
   ],
   "source": [
    "for key, value in meta.items(): \n",
    "    print(key + ': ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, some of these are not very useful for this particular document, but note a really important one: `gid` -- this is the Gutenberg book ID and is a unique identifier for each book. We'll use this a lot later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Spark RDDs\n",
    "In this section we'll learn how to manipulate the Spark \"Resilient Distributed Dataset\" (RDD) object to gain insight into the data and perform calculations. RDDs are the basic building blocks of any Spark application so it is important to learn about their basic features even if later on we will use higher-level (i.e. easier to use) APIs for performing more complex tasks. \n",
    "\n",
    "### First bit of analysis -- how long are the documents? \n",
    "\n",
    "Lets have a quick look at the number of words in these books. We'll do this as follows: \n",
    "\n",
    "0. remove all non-text characters from the text (including punctuation and superflous white spaces)\n",
    "1. take the values (text) of the `text_rdd` (there is a `values()` method that discards the keys of the RDD)\n",
    "2. use the `split` method of the text string to break it up into a list of words\n",
    "3. use `len` to get the length of the list of words. \n",
    "4. finally, we will use the `collect` method to extract the numbers to the driver. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hint**: whenever you are not sure how exactly to proceed, make a new cell and experiment! Use the `first` and `take` methods of the RDD you are creating to see what is happening with the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish \\#1 above, we will use a simple \"regular expression\" to quickly parse the text. Regular expressions are used *whenever* a text-based dataset needs to be cleaned and standardized. They provide very efficient mechanisms for searching and replacing strings. Here's a simple example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use',\n",
       " 'terms',\n",
       " 'Project',\n",
       " 'PG',\n",
       " 'list',\n",
       " 'General',\n",
       " 'General',\n",
       " 'Waller',\n",
       " 'right',\n",
       " 'spring']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# make a regular expression that will find all words that follow the word \"the\"\n",
    "the_re = re.compile(\"the\\s(\\w+)\\s\")\n",
    "\n",
    "# use the regular expression to search the text we retrieved a few cells above (only showing the first ten words)\n",
    "the_re.findall(text)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string inside `re.compile()` above is called a \"regular expression\". A regular expression matches a string to its pattern from left to right. To deconstruct it, here's what it is doing: \n",
    "\n",
    "1. look for the characters \"the\" followed by a single \"white-space\" character (this is what `\\s` means; it includes tabs, newlines, etc.)\n",
    "2. after the space, see if there are more than one alphanumeric characters (`\\w+`) followed by another space. If so, then return this sequence of characters between one whitespace and the other as a \"hit\"\n",
    "\n",
    "If you want to play around with regular expressions, try the handy [pythex regular expression tester](http://pythex.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we pre-define the `no_punctuation` regular expression which can be used to remove all punctuation characters by using its `.sub` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we make a function `clean_rdd` that takes an RDD of `(metadata,raw_text)` tuples and returns an RDD of `(metadata, text)`, \n",
    "# where `text` is just `raw_text` but without any punctuation. The actual action that transformes the string\n",
    "# is done via a helper function `clean_text`. \n",
    "\n",
    "import re\n",
    "# this regex matches everything except for numbers, letters, single white-spaces, apostrophes, and hyphens\n",
    "no_punctuation = re.compile(\"[^a-zA-Z0-9\\s'-]\") \n",
    "\n",
    "def clean_text(args):\n",
    "    metadata, text = args\n",
    "    return metadata, no_punctuation.sub('', text)\n",
    "\n",
    "def clean_rdd(rdd): \n",
    "    return rdd.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_text_rdd = clean_rdd(text_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: use the `map` method of clean_text_rdd transform the text into the length of the string\n",
    "text_lengths = (clean_text_rdd.values()\n",
    "                              .map(len)\n",
    "                              .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[323739,\n",
       " 144550,\n",
       " 109600,\n",
       " 281412,\n",
       " 118049,\n",
       " 1026636,\n",
       " 199093,\n",
       " 89397,\n",
       " 750640,\n",
       " 752102]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lengths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these valuse we can now quickly plot a histogram to get an idea of text lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAGVCAYAAADkEdnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aecb10dc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(text_lengths, bins = 100, range=[0,1e6]);\n",
    "plt.xlabel('$N_{words}$'); plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variables: a good way to distribute metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our dataset at the moment is composed of `(dictionary, text)` tuples. This is not ideal for several reasons: first, complex structures like dictionaries are more expensive to serialize, so replacing them by simple integer IDs can improve performance. And second, dictionaries are not hashable so cannot serve as keys. \n",
    "\n",
    "The solution is to replace the dictionary with just a single ID integer (in this case we already have it, the `gid` or the gutenberg ID of the book). However, as we manipulate the dataset, we will still want ready access to the metadata, but at the moment we have no efficient way of working with it. What we want is to generate a lookup table of metadata that can easily be retrieved when needed. \n",
    "\n",
    "For these kinds of situations when passing around some data to all the workers is required, you should use Spark's [broadcast variable](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) mechanism. By placing data inside a broadcast variable, you make it available to all the workers and it only needs to be sent across the network once. The next time a value from the broadcast variable is needed, it's simply taken from the local executor's memory, incurring no extra network cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.46 s, sys: 308 ms, total: 1.76 s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "meta_dict = dict()\n",
    "\n",
    "for meta in text_rdd.keys().toLocalIterator() :\n",
    "    meta_dict[meta['gid']] = meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a look-up table that allows us to quickly access all the metadata indexed by `gid`. For example to get the metadata of book with `gid=101`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author_id': '66',\n",
       " 'author_name': ['Sterling', ' Bruce'],\n",
       " 'birth_year': '1954',\n",
       " 'death_year': None,\n",
       " 'downloads': '196',\n",
       " 'file_types': {'101-h.htm': 'text/html; charset=iso-8859-1',\n",
       "  '101.epub.images': 'application/epub+zip',\n",
       "  '101.epub.noimages': 'application/epub+zip',\n",
       "  '101.kindle.images': 'application/x-mobipocket-ebook',\n",
       "  '101.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "  '101.rdf': 'application/rdf+xml',\n",
       "  '101.txt': 'text/plain; charset=us-ascii',\n",
       "  '101.txt.utf-8': 'text/plain',\n",
       "  '101.zip': 'text/plain; charset=us-ascii'},\n",
       " 'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/0/101/101.txt',\n",
       " 'first_name': 'Bruce',\n",
       " 'gid': '101',\n",
       " 'language': 'en',\n",
       " 'last_name': 'Sterling',\n",
       " 'license': 'Copyrighted. Read the copyright notice inside this book for details.',\n",
       " 'subtitle': '',\n",
       " 'title': 'The Hacker Crackdown: Law and Disorder on the Electronic Frontier'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict['101']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get, for example, the author birth year for book with `gid = 101`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1954'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict['101']['birth_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to send this lookup-table to all the executors, so turn it into a broadcast variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.setLogLevel('INFO')\n",
    "# call it meta_b for 'broadcast'\n",
    "meta_b = sc.broadcast(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`meta_b` is a \"broadcast variable\" - this means that Spark has taken the object we gave it (`meta_dict`), shipped it to all the workers, and provided us with a new object that we can use to access that data which we named `meta_b`. \n",
    "\n",
    "The underlying data stored in `meta_b` can be accessed simply by\n",
    "\n",
    "    > meta_b.value\n",
    "    \n",
    "We'll make use of this soon. If you check the console/terminal output, you will see an INFO message that the broadcast has been created, i.e. \n",
    "\n",
    "```\n",
    "15/06/24 17:18:44 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 910.7 KB, free 4.1 GB)\n",
    "15/06/24 17:18:44 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.201.20.22:47821 (size: 910.7 KB, free: 4.1 GB)\n",
    "15/06/24 17:18:44 INFO spark.SparkContext: Created broadcast 6 from broadcast at PythonRDD.scala:403\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the metadata dictionary for later use\n",
    "We will need the metadata dictionary at a later point, so we save it to disk now to avoid having to regenerate it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('{home}/gutenberg_metadata.dump'.format(home=os.environ['HOME']), 'wb') as f:\n",
    "    pickle.dump(meta_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saved it to your home directory into a file called `gutenberg_metadata.dump`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data with filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to do some quality checks on the data. Let's check out the first couple of metadata entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'author_id': '116',\n",
       "  'author_name': ['Various'],\n",
       "  'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': '13',\n",
       "  'file_types': {'13637-8.txt': 'text/plain; charset=iso-8859-1',\n",
       "   '13637-h.htm': 'text/html; charset=iso-8859-1',\n",
       "   '13637-h.zip': 'text/html; charset=iso-8859-1',\n",
       "   '13637.epub.images': 'application/epub+zip',\n",
       "   '13637.epub.noimages': 'application/epub+zip',\n",
       "   '13637.kindle.images': 'application/x-mobipocket-ebook',\n",
       "   '13637.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "   '13637.rdf': 'application/rdf+xml',\n",
       "   '13637.txt': 'text/plain; charset=us-ascii',\n",
       "   '13637.txt.utf-8': 'text/plain'},\n",
       "  'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/3/13637/13637-8.txt',\n",
       "  'first_name': None,\n",
       "  'gid': '13637',\n",
       "  'language': 'en',\n",
       "  'last_name': 'Various',\n",
       "  'license': 'Public domain in the USA.',\n",
       "  'subtitle': '',\n",
       "  'title': \"McClure's Magazine, Vol. 6, No. 2, January, 1896\"},\n",
       " {'author_id': '116',\n",
       "  'author_name': ['Various'],\n",
       "  'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': '7',\n",
       "  'file_types': {'13638-8.txt': 'text/plain; charset=iso-8859-1',\n",
       "   '13638-8.zip': 'text/plain; charset=iso-8859-1',\n",
       "   '13638-h.htm': 'text/html; charset=iso-8859-1',\n",
       "   '13638.epub.images': 'application/epub+zip',\n",
       "   '13638.epub.noimages': 'application/epub+zip',\n",
       "   '13638.kindle.images': 'application/x-mobipocket-ebook',\n",
       "   '13638.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "   '13638.rdf': 'application/rdf+xml',\n",
       "   '13638.txt': 'text/plain; charset=us-ascii',\n",
       "   '13638.txt.utf-8': 'text/plain',\n",
       "   '13638.zip': 'text/plain; charset=us-ascii'},\n",
       "  'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/3/13638/13638-8.txt',\n",
       "  'first_name': None,\n",
       "  'gid': '13638',\n",
       "  'language': 'en',\n",
       "  'last_name': 'Various',\n",
       "  'license': 'Public domain in the USA.',\n",
       "  'subtitle': '',\n",
       "  'title': 'Notes and Queries, Number 19, March 9, 1850'},\n",
       " {'author_id': '116',\n",
       "  'author_name': ['Various'],\n",
       "  'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': '22',\n",
       "  'file_types': {'13639-8.txt': 'text/plain; charset=iso-8859-1',\n",
       "   '13639-8.zip': 'text/plain; charset=iso-8859-1',\n",
       "   '13639-h.htm': 'text/html; charset=iso-8859-1',\n",
       "   '13639.epub.images': 'application/epub+zip',\n",
       "   '13639.epub.noimages': 'application/epub+zip',\n",
       "   '13639.kindle.images': 'application/x-mobipocket-ebook',\n",
       "   '13639.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "   '13639.rdf': 'application/rdf+xml',\n",
       "   '13639.txt': 'text/plain; charset=us-ascii',\n",
       "   '13639.txt.utf-8': 'text/plain'},\n",
       "  'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/3/13639/13639-8.txt',\n",
       "  'first_name': None,\n",
       "  'gid': '13639',\n",
       "  'language': 'en',\n",
       "  'last_name': 'Various',\n",
       "  'license': 'Public domain in the USA.',\n",
       "  'subtitle': '',\n",
       "  'title': 'Punch, or the London Charivari, Volume 1, July 17, 1841'},\n",
       " {'author_id': '116',\n",
       "  'author_name': ['Various'],\n",
       "  'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': '6',\n",
       "  'file_types': {'13640-8.txt': 'text/plain; charset=iso-8859-1',\n",
       "   '13640-h.htm': 'text/html; charset=iso-8859-1',\n",
       "   '13640.epub.images': 'application/epub+zip',\n",
       "   '13640.epub.noimages': 'application/epub+zip',\n",
       "   '13640.kindle.images': 'application/x-mobipocket-ebook',\n",
       "   '13640.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "   '13640.rdf': 'application/rdf+xml',\n",
       "   '13640.txt': 'text/plain; charset=us-ascii',\n",
       "   '13640.txt.utf-8': 'text/plain',\n",
       "   '13640.zip': 'text/plain; charset=us-ascii',\n",
       "   'pg13640.cover.medium.jpg': 'image/jpeg',\n",
       "   'pg13640.cover.small.jpg': 'image/jpeg'},\n",
       "  'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/4/13640/13640-8.txt',\n",
       "  'first_name': None,\n",
       "  'gid': '13640',\n",
       "  'language': 'en',\n",
       "  'last_name': 'Various',\n",
       "  'license': 'Public domain in the USA.',\n",
       "  'subtitle': '',\n",
       "  'title': 'Scientific American Supplement, No. 821, September 26, 1891'},\n",
       " {'author_id': '116',\n",
       "  'author_name': ['Various'],\n",
       "  'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': '3',\n",
       "  'file_types': {'13641-8.txt': 'text/plain; charset=iso-8859-1',\n",
       "   '13641-8.zip': 'text/plain; charset=iso-8859-1',\n",
       "   '13641-h.htm': 'text/html; charset=iso-8859-1',\n",
       "   '13641.epub.images': 'application/epub+zip',\n",
       "   '13641.epub.noimages': 'application/epub+zip',\n",
       "   '13641.kindle.images': 'application/x-mobipocket-ebook',\n",
       "   '13641.kindle.noimages': 'application/x-mobipocket-ebook',\n",
       "   '13641.rdf': 'application/rdf+xml',\n",
       "   '13641.txt': 'text/plain; charset=us-ascii',\n",
       "   '13641.txt.utf-8': 'text/plain'},\n",
       "  'filename': '/cluster/home/roskarr/projects/gutenberg_data/gutenberg_text/1/3/6/4/13641/13641-8.txt',\n",
       "  'first_name': None,\n",
       "  'gid': '13641',\n",
       "  'language': 'en',\n",
       "  'last_name': 'Various',\n",
       "  'license': 'Public domain in the USA.',\n",
       "  'subtitle': '',\n",
       "  'title': 'The American Missionary  Volume 42, No. 10, October, 1888'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at just the first few entries it becomes clear that we're going to have to do some quality control here. For example, we probably don't want books with \"None\" as either of the author names, and likewise we have to have the birth date in order to be able to create a time series out of the data in the end. \n",
    "\n",
    "Construct an RDD, as above, except that you filter out all the elements that have `None` for `title`, `first_name`, `last_name`, or `birth_year`. In addition, filter out the data with \"BC\" in either birth or death year. \n",
    "\n",
    "As a reminder, here is a cartoon illustration of the difference between `map` and `filter` RDD methods. `map` simply applies the function to each element, returning another element. \n",
    "\n",
    "![map](../figs/map_example.svg)\n",
    "\n",
    "In this example, with `filter` we are filtering out all the even elements of the RDD. The function that is passed to `filter` just has to evaluate to either `True` (1) or `False` (0) given the input data. The function `lambda (k,v): v%2` evaluates to 0 if `v` is even and 1 of `v` is odd. Hence, only the odd values pass the filter. \n",
    "\n",
    "![filter](../figs/filter_example.svg)\n",
    "\n",
    "The `filter_func` has already been defined for you below, but you need to apply it to `text_rdd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_func(meta) : \n",
    "    no_none = all([meta[name] is not None for name in ['title', 'first_name', 'last_name', 'birth_year']])\n",
    "    if not no_none : \n",
    "        return False\n",
    "    else : \n",
    "        no_birth_bc = 'BC' not in meta['birth_year']\n",
    "        no_death_bc = True if meta['death_year'] is None else 'BC' not in meta['death_year']\n",
    "        return no_birth_bc & no_death_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: filter the meta data of the RDD using the filter_func \n",
    "# remember that the data of text_rdd is a (metadata, text) tuple -- filter_func only needs the metadata\n",
    "filtered_rdd = text_rdd.filter(lambda (meta, text): filter_func(meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have left? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfiltered = filtered_rdd.count()\n",
    "print('number of books after filtering: ', nfiltered)\n",
    "assert(nfiltered == 34123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A final bit of cleanup: \n",
    "\n",
    "some of the books end up split across multiple entries. Since it's the same book, each of the entries should have the same `gid`. \n",
    "\n",
    "To check for this we will use one of the most basic and common MapReduce patterns -- the key count: \n",
    "\n",
    "* map the data into `key`,`value` pairs where `key` is the quantity we want to count and `value` is just 1. In this case, the `key` will be `gid`\n",
    "* invoke a reduction *by key*, where the reduction operator is a simple addition\n",
    "\n",
    "Finally, we will sort the result in descending order and print out the first few elements to check whether we have to worry about documents spanning multiple files or not. \n",
    "\n",
    "The RDD operations that are needed are [`reduceByKey`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) and [sortBy](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy).\n",
    "\n",
    "`reduceByKey` works by grouping all data of a key together and applying the reduction function just to that data. Here's a simple illustration, in this case using a simple addition of two elements as a reduction:\n",
    "\n",
    "![reducebykey](../../slides/figs/reduceByKey_example.svg)\n",
    "\n",
    "\n",
    "\n",
    "For the `keyFunc` of the call to `sortBy`, use a `lambda` function that extracts the counts obtained from the `reduceByKey`. \n",
    "\n",
    "So, the procedure should be : \n",
    "\n",
    "1. `map` the `filtered_rdd` using a lambda function to contain (`gid`, 1) tuples\n",
    "2. `reduceByKey`\n",
    "3. `sortBy` (specify decreasing order, see the API) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map the filtered_rdd to contain just the tuple (gid, 1)\n",
    "map_filtered = filtered_rdd.map(lambda (meta, text): (meta['gid'],1))\n",
    "\n",
    "# reduce the map_filtered rdd by key using the add operator to get the total counts per gid\n",
    "reduced_gid_rdd = map_filtered.reduceByKey(<FILL IN>)\n",
    "\n",
    "# sort by count and print out the top 5\n",
    "reduced_gid_rdd.sortBy(<FILL IN>).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(_ == [(30310, 51), (6478, 43), (3772, 40), (8700, 35), (3332, 33)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are several transformations here that lead to the final result, `sorted_reduced`. A common syntax is to group them all together, by enclosing them in `( )` and chaining them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: repeat the above steps but chain them together\n",
    "(filtered_rdd.<FILL IN>\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a few that are made up of multiple sections. To combine them, we will use `reduceByKey` which will result in having an RDD of `gid`'s as keys and the combined text of each `gid`. The reduction function in `reduceByKey` can be a simple in-line lambda function that just adds two elements together -- in this case we'll use the `add` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, map filtered_rdd to contain tuples (gid, text)\n",
    "# second, use reduceByKey to combine together text belonging to the same gid\n",
    "cleaned_rdd = (filtered_rdd.map(lambda (meta, text): (meta['gid'], text))\n",
    "                           .reduceByKey(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple sanity check, lets look at `gid`=6478, which according to the cell above has 43 sections in the original dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(filtered_rdd.map(lambda (meta, text): (meta['gid'],1))\n",
    "                .lookup(6478))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cleaned_rdd.lookup(6478))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to do all these pre-processing steps again at a later point, lets also save the `cleaned_rdd`. \n",
    "\n",
    "**In the cells below, replace \"YOUR USERNAME\" with your own username.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_rdd.saveAsPickleFile('/user/YOUR USERNAME/gutenberg/cleaned_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now saved in the directory we specified, one file per partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hadoop fs -ls /user/YOUR USERNAME/gutenberg/cleaned_rdd | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hadoop fs -du -h /user/YOUR USERNAME/gutenberg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we used the `hadoop` command in the local bash shell (the `!` at the beginning of the line means we are executing the command in the shell). This allows us to access the hadoop filesystem (HDFS), which is separate from the local file system we are used to. You'll notice, for example, that this directory doesn't exist in the *local* filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls /user/YOUR USERNAME/gutenberg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also browse the filesystem via the [HDFS web UI](http://hadoop.ethz.ch:50070). The `hadoop fs` command has many of the same options as regular Linux/Unix shell commands you might be used to for manipulating files and directories. Try running\n",
    "\n",
    "```bash\n",
    "cluster $> module load hadoop\n",
    "cluster $> hadoop fs -help\n",
    "```\n",
    "\n",
    "in a new shell to see all the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of steps up until this point\n",
    "\n",
    "We've done quite a lot already with our dataset in Spark, although it's only the beginning!\n",
    "\n",
    "1. created an RDD of filenames (`filename_rdd`)\n",
    "2. transformed the `filename_rdd` into an RDD of `(metadata, text)` (`text_rdd`); we also saved this to HDFS\n",
    "3. filtered out data with bad metadata, e.g. missing author names etc.\n",
    "3. cleaned up the entries a bit more by merging ones with identical IDs; we called this `cleaned_rdd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting down the `SparkContext`\n",
    "\n",
    "Now that the pre-processing is done, we will shut down the `SparkContext` before continuing to the data analysis notebook. We have all of our results saved in HDFS, so to continue from where we left off will just require loading data from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pre-processing steps are complete, we can continue to the [analysis notebook](part2-ngram-viewer-EMPTY.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
