{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Gutenberg Books Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the cleaned, pre-processed data that we created in the [pre-processing part](gutenberg-preprocessing-SOLUTIONS.ipynb). As a reminder, we ended up with an RDD of `(gid, text)` tuples that has been cleaned and we stored it on HDFS at `/user/<YOUR_USERNAME>/gutenberg/cleaned_rdd`. \n",
    "\n",
    "In the [first analysis notebook](gutenberg-analysis-SOLUTIONS.ipynb) we build an N-gram viewer for the gutenberg books project. Now, we will use the corpus to train a simple language classification model using [Spark's machine learning library](http://spark.apache.org/docs/latest/mllib-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python and Spark\n",
    "\n",
    "These steps are identical to those used in the previous notebook so we have omitted the lengthy explanations -- if you need to check what any of this is doing, have a look at the pre-processing notebook. }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark, os\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the number of executors and cores into variables so we can refer to it later\n",
    "num_execs = 10\n",
    "exec_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initializing the SparkConf\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '4g'\n",
    "os.environ['SPARK_CONF_DIR'] = '%s/../../spark_config'%os.getcwd()\n",
    "conf = (SparkConf()\n",
    "            .set('spark.executor.memory', '8g')\n",
    "            .set('spark.executor.instances', str(num_execs))\n",
    "            .set('spark.executor.cores', str(exec_cores))\n",
    "            .set('spark.storage.memoryFraction', 0.3)\n",
    "            .set('spark.shuffle.memoryFraction', 0.5)\n",
    "            .set('spark.yarn.executor.memoryOverhead', 3072)\n",
    "            .set('spark.yarn.am.memory', '8g')\n",
    "            .set('spark.yarn.am.cores', 4)\n",
    "            .set('spark.executorEnv.PYTHONPATH', \n",
    "                 '{home}/spark_workshop/notebooks/gutenberg'.format(home=os.environ['HOME']))\n",
    "            .set('spark.executorEnv.PATH', os.environ['PATH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'yarn-client', conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works successfully, you can check the [YARN application scheduler](http://hadoop.hpc-net.ethz.ch:8088/cluster) and you should see your app listed there. Clicking on the \"Application Master\" link will bring up the familiar Spark Web UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: load cleaned_rdd from the HDFS\n",
    "cleaned_rdd = sc.pickleFile('/user/roskarr/gutenberg/cleaned_rdd').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the metadata dictionary and broadcast it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cPickle import load\n",
    "\n",
    "with open('{home}/gutenberg_metadata.dump'.format(home=os.environ['HOME']), 'r') as f :\n",
    "    meta_dict = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: create meta_b by broadcasting meta_dict\n",
    "meta_b = sc.broadcast(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our `cleaned_rdd` contains `gid`'s as keys and text as values and if we want some other piece of metadata, we can just access it via the lookup table, for example `meta_b.value[gid][meta_name]`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same `extract_ngrams` and `vectorize_doc` functions as in the previous notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "\n",
    "def extract_ngrams(tokens, ngram_range=[1,1], select_ngrams = None, character = False):\n",
    "    \"\"\"\n",
    "    Turn tokens into a sequence of n-grams \n",
    "\n",
    "    **Inputs**:\n",
    "\n",
    "    *tokens*: a list of tokens\n",
    "\n",
    "    **Optional Keywords**:\n",
    "\n",
    "    *ngram_range*: a tuple with min, max ngram ngram_range\n",
    "    \n",
    "    *select_ngrams*: the vocabulary to use\n",
    "    \n",
    "    *character*: True if using character ngrams; default is False\n",
    "\n",
    "    **Output**\n",
    "\n",
    "    Generator yielding a list of ngrams in the desired range\n",
    "    generated from the input list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    join_str = \"\" if character else \" \"\n",
    "    \n",
    "    # handle token n-grams\n",
    "    min_n, max_n = ngram_range\n",
    "    n_tokens = len(tokens)\n",
    "    for n in xrange(min_n, min(max_n + 1, n_tokens + 1)):\n",
    "        for i in xrange(n_tokens - n + 1):\n",
    "            if n == 1: \n",
    "                res = tokens[i]\n",
    "            else : \n",
    "                res = \"\".join(tokens[i: i+n])\n",
    "           \n",
    "            if select_ngrams is not None : \n",
    "                if res in select_ngrams: \n",
    "                    yield res\n",
    "            else : \n",
    "                yield res\n",
    "            \n",
    "def vectorize_doc(doc, vocab, ngram_range = [1,1]) : \n",
    "    \"\"\"\n",
    "    Returns a vector representation of `doc` given the reference \n",
    "    vocabulary `vocab` after tokenizing it with `tokenizer`\n",
    "    \n",
    "    Arguments: \n",
    "        \n",
    "        doc: a sequence of tokens (words or characters)\n",
    "        \n",
    "        vocab: the vocabulary mapping\n",
    "        \n",
    "        ngram_range: the range of ngrams to process\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        a sparse vector representation of the document given the vocabulary mapping\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    from scipy.sparse import csr_matrix \n",
    "    \n",
    "    d = defaultdict(int)\n",
    "    \n",
    "    for ngram in extract_ngrams(doc, ngram_range, vocab) : \n",
    "        d[ngram] += 1\n",
    "        \n",
    "    values = np.empty(len(d))\n",
    "    indices = np.empty(len(d))\n",
    "    \n",
    "    for i, (ngram, val) in enumerate(d.iteritems()) : \n",
    "        indices[i] = vocab[ngram]\n",
    "        values[i] = val\n",
    "        \n",
    "    return csr_matrix((values, (indices, np.zeros(len(d)))), shape = (len(vocab), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language classification\n",
    "\n",
    "Here we will try to use some of the same techniques we developed before, but apply them to a classification problem: determining whether a text is English or German. \n",
    "\n",
    "We will use the rather straightforward method outlined in [Cavnar & Trenkle 1994](http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf):\n",
    "\n",
    "For each of the English/German training sets:\n",
    "\n",
    "1. tokenize the text (spaces are also tokens, so we replace them with \"_\")\n",
    "2. extract N-grams where 1 < N < 5\n",
    "3. determine 300 most common N-grams for the whole corpus\n",
    "4. encode both sets of documents using the combined top 300-ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we used words as \"tokens\" -- now we will use characters, even accounting for white space (which we will replace with \"_\"). We will use the two example sentences again:\n",
    "\n",
    "    document 1: \"a dog bit me\"\n",
    "    document 2: \"i bit the dog back\"\n",
    "    \n",
    "First, we use the `extract_ngrams` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"a dog bit me\"\n",
    "s2 = \"i bit the dog back\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngrams1 = list(extract_ngrams(s1.replace(' ','_'), ngram_range=[1,5], character=True))\n",
    "ngrams2 = list(extract_ngrams(s2.replace(' ','_'), ngram_range=[1,5], character=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '_', 'd', 'o', 'g', '_', 'b', 'i', 't', '_', 'm', 'e', 'a_', '_d', 'do', 'og', 'g_', '_b', 'bi', 'it', 't_', '_m', 'me', 'a_d', '_do', 'dog', 'og_', 'g_b', '_bi', 'bit', 'it_', 't_m', '_me', 'a_do', '_dog', 'dog_', 'og_b', 'g_bi', '_bit', 'bit_', 'it_m', 't_me', 'a_dog', '_dog_', 'dog_b', 'og_bi', 'g_bit', '_bit_', 'bit_m', 'it_me']\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can create the vocabulary by getting the set of all ngrams and building a lookup table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set(list(ngrams1)) | set(list(ngrams2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ngrams:  89\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = {word:ind for ind,word in enumerate(vocab)}\n",
    "print('number of ngrams: ',len(vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that extracting ngrams can increase the size of the data quite a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_': 73,\n",
       " '_b': 24,\n",
       " '_ba': 12,\n",
       " '_bac': 19,\n",
       " '_back': 27,\n",
       " '_bi': 8,\n",
       " '_bit': 29,\n",
       " '_bit_': 54,\n",
       " '_d': 23,\n",
       " '_do': 55,\n",
       " '_dog': 83,\n",
       " '_dog_': 72,\n",
       " '_m': 18,\n",
       " '_me': 84,\n",
       " '_t': 15,\n",
       " '_th': 71,\n",
       " '_the': 36,\n",
       " '_the_': 31,\n",
       " 'a': 74,\n",
       " 'a_': 66,\n",
       " 'a_d': 34,\n",
       " 'a_do': 58,\n",
       " 'a_dog': 88,\n",
       " 'ac': 5,\n",
       " 'ack': 62,\n",
       " 'b': 41,\n",
       " 'ba': 68,\n",
       " 'bac': 67,\n",
       " 'back': 25,\n",
       " 'bi': 70,\n",
       " 'bit': 37,\n",
       " 'bit_': 6,\n",
       " 'bit_m': 35,\n",
       " 'bit_t': 30,\n",
       " 'c': 75,\n",
       " 'ck': 0,\n",
       " 'd': 42,\n",
       " 'do': 26,\n",
       " 'dog': 81,\n",
       " 'dog_': 7,\n",
       " 'dog_b': 9,\n",
       " 'e': 76,\n",
       " 'e_': 64,\n",
       " 'e_d': 49,\n",
       " 'e_do': 21,\n",
       " 'e_dog': 51,\n",
       " 'g': 77,\n",
       " 'g_': 14,\n",
       " 'g_b': 59,\n",
       " 'g_ba': 63,\n",
       " 'g_bac': 22,\n",
       " 'g_bi': 65,\n",
       " 'g_bit': 3,\n",
       " 'h': 44,\n",
       " 'he': 38,\n",
       " 'he_': 4,\n",
       " 'he_d': 50,\n",
       " 'he_do': 11,\n",
       " 'i': 78,\n",
       " 'i_': 60,\n",
       " 'i_b': 57,\n",
       " 'i_bi': 28,\n",
       " 'i_bit': 85,\n",
       " 'it': 56,\n",
       " 'it_': 17,\n",
       " 'it_m': 53,\n",
       " 'it_me': 33,\n",
       " 'it_t': 43,\n",
       " 'it_th': 69,\n",
       " 'k': 79,\n",
       " 'm': 80,\n",
       " 'me': 39,\n",
       " 'o': 82,\n",
       " 'og': 47,\n",
       " 'og_': 40,\n",
       " 'og_b': 2,\n",
       " 'og_ba': 13,\n",
       " 'og_bi': 16,\n",
       " 't': 48,\n",
       " 't_': 1,\n",
       " 't_m': 61,\n",
       " 't_me': 32,\n",
       " 't_t': 87,\n",
       " 't_th': 10,\n",
       " 't_the': 52,\n",
       " 'th': 20,\n",
       " 'the': 86,\n",
       " 'the_': 46,\n",
       " 'the_d': 45}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can use `vectorize_doc` with the vocabulary mapping to turn the documents into vectors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,\n",
       "        1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  3.,  1.,  0.,  1.,  1.,\n",
       "        1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_doc(s1.replace(' ','_'), vocab_dict, ngram_range=[1,5]).toarray().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving memory consumption with `mapPartitions`\n",
    "\n",
    "As you can see from the simple example above, by extracting the 1-5 grams from a simple 12-character string, we created a vector with 48 stored values. Our actual documents will therefore swell in size very rapidly -- we definitely don't want to be holding all of those huge lists in memory! \n",
    "\n",
    "What we want in the first part is to get the top most-used N-grams. For this, we just need to create an RDD of N-grams and to avoid building lists we'll use the technique of generators discussed on the first day. \n",
    "\n",
    "Note that the `extract_ngrams` function above is already a generator: now we just want to make a small wrapper function that uses `extract_ngrams` to \"yield\" ngrams one by one into the RDD. \n",
    "\n",
    "A slight complication is that `mapPartitions` gives us an *iterator* over the data in the partition - the items returned by this iterator will be just individual documents, which we can then pass to `extract_ngrams`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def ngram_generator(iterator, ngram_range=[1,1]) : \n",
    "    \"\"\"Take an iterator of documents and create a generator of ngrams\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "        iterator: the document iterator\n",
    "        \n",
    "    Keywords:\n",
    "        \n",
    "        ngram_range: the range of ngrams to consider\n",
    "    \"\"\" \n",
    "    for text in iterator : \n",
    "        for ngram in extract_ngrams(text.replace(' ', '_'), ngram_range): \n",
    "            yield ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will subsample the `cleaned_rdd` in order to make this next set of cells complete in a reasonable amount of time -- once it's working, you can go back and do it for the full dataset, but it will take approximately 30 minutes. Note that we also repartition the data in order to ease the resource requirements of individual partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_data = cleaned_rdd.sample(False, 0.1).repartition(2000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_rdd = sampled_data.filter(lambda (gid,text): (meta_b.value[gid]['lang'] == 'en')).cache()\n",
    "german_rdd = sampled_data.filter(lambda (gid, text): (meta_b.value[gid]['lang'] == 'de')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range = [1,3] # should use 1-5 ngram range, but make it smaller to speed up the processing a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the sets of most common english and german ngrams\n",
    "\n",
    "To build the sets of ngrams, we use the now-familiar pattern: \n",
    "\n",
    "1. map the documents in the RDDs to their constituent ngrams (here we use the mapPartition call with the `ngram_generator` defined above)\n",
    "2. do the distributed key count using the `map` --> `(key, 1)` --> `reduceByKey` pattern\n",
    "3. sort the result (in descending order) and take the top 1000 ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: \n",
    "en_ngram_counts = (english_rdd.values()\n",
    "                              .mapPartitions(lambda it: ngram_generator(it, ngram_range))\n",
    "                              .map(lambda ngram: (ngram,1))\n",
    "                              .reduceByKey(lambda a,b:a+b).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "de_ngram_counts = (german_rdd.values()\n",
    "                             .mapPartitions(lambda it: ngram_generator(it, ngram_range))\n",
    "                             .map(lambda ngram: (ngram,1))\n",
    "                             .reduceByKey(lambda a,b:a+b).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 670 ms, sys: 158 ms, total: 828 ms\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_1000_en_ngrams = (en_ngram_counts.sortBy(lambda (ngram,count): count, False)\n",
    "                                     .map(lambda (ngram, count): ngram)\n",
    "                                     .take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 645 ms, sys: 151 ms, total: 796 ms\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_1000_de_ngrams = (de_ngram_counts.sortBy(lambda (ngram,count): count, False)\n",
    "                                     .map(lambda (ngram, count): ngram)\n",
    "                                     .take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building the top_ngrams dictionaries\n",
    "# combine the german and english ngrams\n",
    "top_ngrams = set(top_1000_de_ngrams) | set(top_1000_en_ngrams)\n",
    "\n",
    "# build the ngrams dictionary lookup\n",
    "top_ngrams_dict = {ngram:i for (i,ngram) in enumerate(top_ngrams)}\n",
    "\n",
    "# broadcast the ngrams dictionary\n",
    "top_ngrams_dict_b = sc.broadcast(top_ngrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: map the sampled_data RDD into (gid, vector) tuples \n",
    "# by using the vectorize_doc function and the broadcasted ngram dictionary\n",
    "\n",
    "vector_rdd = sampled_data.map(lambda (gid,text): (gid, \n",
    "                                                 vectorize_doc(text.replace(' ','_'),top_ngrams_dict_b.value, ngram_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ngram_generator` function followed by the `map` and `reduceByKey` calls above is clear but a bit inefficient -- can you transfer some of the reduction into `mapPartition` and `ngram_generator`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the language classification model\n",
    "\n",
    "To train the model, we need to first map the `vector_rdd` elements into [`LabeledPoint`](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LabeledPoint), which is just a Spark abstraction that encompases a *label* and a *vector*. We then split the data into a training and validation sets and produce the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a `vector_lp` RDD by mapping the contents of `vector_rdd` into a `LabeledPoint` using 0 if the language is english and 1 if it is anything else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create an RDD of LabeledPoint with 0 for english and 1 for german\n",
    "vector_lp = vector_rdd.map(lambda (gid, vec): LabeledPoint(0 if meta_b.value[gid]['lang'] == 'en' else 1, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark machine learning library provides a simple method for creating training and validation sets, which we will use below. These will be our inputs for the logistic regression model fitting -- it is *always* a good idea to cache the inputs, since the training requires many iterations over the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[42] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training, validation = vector_lp.randomSplit([0.7,0.3])\n",
    "training.cache()\n",
    "validation.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the training set to the model\n",
    "\n",
    "Here we will use the basic logistic regression model with stochastic gradient descent -- feel free to experiment with different parameters and other models from [pyspark.mllib.classification](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithSGD.train(training, regType='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the performance of the model, we define a function that takes a data RDD and a model as parameters and computes the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_model(data, model) : \n",
    "    \"\"\"Calculates the model error on the data\n",
    "    \n",
    "    Arguments: \n",
    "        \n",
    "        data: the data RDD\n",
    "        \n",
    "        model: the classification model\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        the error, which is the fraction of incorrectly predicted elements\n",
    "    \"\"\"\n",
    "    \n",
    "    error = (data.map(lambda p: (p.label, model.predict(p.features)))\n",
    "                 .filter(lambda (v,p): v!=p).count())/float(data.count())\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.00856793145655\n",
      "Validation Error = 0.0061919504644\n"
     ]
    }
   ],
   "source": [
    "train_error = check_model(training, model)\n",
    "print(\"Training Error = \" + str(train_error))\n",
    "validation_error = check_model(validation, model)\n",
    "print(\"Validation Error = \" + str(validation_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, lets create a function that will score a new string of text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_language(text, model) : \n",
    "    \"\"\"Predict the language given the pre-trained model\n",
    "    \n",
    "    Arguments: \n",
    "        text: a string\n",
    "        \n",
    "        model: the trained logistic regression model\n",
    "    \"\"\"\n",
    "    vec = vectorize_doc(text.replace(' ','_'), top_ngrams_dict_b.value, [1,2])\n",
    "    \n",
    "    return model.predict(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out! Enter your own sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"a dog bit me!\"\n",
    "predict_language(text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
