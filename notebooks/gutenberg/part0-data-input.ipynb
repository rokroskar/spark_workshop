{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "import os, subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg N-Grams\n",
    "\n",
    "In this series of notebooks, we will quantitatively explore the text of the [Gutenberg E-Books Project](https://www.gutenberg.org/), a free repository of e-books that are in the public domain. small python package has been created that allows you to easily parse the text and the associated metadata. \n",
    "\n",
    "In this part \"zero\" notebook, we just ingest the data, process the text, and save the raw text RDD (with punctuation and html tags removed). \n",
    "\n",
    "\n",
    "## Raw data setup\n",
    "\n",
    "### The books\n",
    "To begin, download the DVD image using a torrent client and mount it on your system. See the instructions on the [Gutenberg DVD page](http://www.gutenberg.org/wiki/Gutenberg:The_CD_and_DVD_Project)).\n",
    "\n",
    "### The metadata\n",
    "The metadata (things like author birth date, language etc.) are stored in a series of '.rdf' files, which need to be [downloaded separately](http://www.gutenberg.org/wiki/Gutenberg:Feeds). Once you download the `rdf-files.tar.gz`, untar and unzip it into a directory on your computer. \n",
    "\n",
    "In the cell below, set the `rdf_path` to where you extracted the metadata, `data_path` to where the DVD volume is mounted, and `extract_path` where the text of all the books will be extracted to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdf_path = '/Users/rok/gutenberg_data/cache/epub'\n",
    "data_path = '/Volumes/PGDVD_2010_04_RC2/'\n",
    "extract_path = '/Users/rok/gutenberg_data/new_dload/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the text\n",
    "\n",
    "First, we use the `gutenberg_cleanup` code to extract all data from the zip files located in the DVD archive. This will take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gutenberg_cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gutenberg_cleanup.extract_data(data_path, extract_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting raw data into Spark\n",
    "\n",
    "With the raw data on disk, we are ready to start processing it in Spark. \n",
    "\n",
    "### Spark configuration\n",
    "\n",
    "Below we specify that this notebook should use the configuration stored in <code>./spark_config</code> -- the options will be discussed in detail in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note that the environment variables have to be declared before any other spark initialization takes place (including creating a <code>SparkConf</code> object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ncores = int(os.environ.get('LSB_DJOB_NUMPROC', 1))\n",
    "\n",
    "# os.environ['SPARK_CONF_DIR'] = os.path.abspath('./spark_config')\n",
    "# os.environ['SPARK_DRIVER_MEMORY'] = '%dG'%(ncores*2*0.7)\n",
    "# os.environ['PYSPARK_PYTHON'] = subprocess.check_output('which python', shell=True).rstrip()\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the `SparkContext`\n",
    "This is our entry point to the Spark runtime - it is used to push data into spark or load RDDs from disk etc. If you are running in a hadoop environment, set the `master` keyword in `SparkContext` to 'yarn-client' - otherwise use the 'local[\\*]' master, which will run spark locally on all available cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works successfully, you can check UI at the URL listed in the cell below (run it first): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://129.132.179.130:4040'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a key-value RDD of book metadata and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data into spark from a collection of local files is a very common task. A useful pattern to keep in mind is the following: \n",
    "\n",
    "1. make a list of filenames and distribute it among the workers\n",
    "3. \"map\" each filename to the data you want to get out\n",
    "4. now you are left with the RDD of raw data distributed among the workers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case of the Gutenberg Project e-book data, we have `.zip` files which hold the actual book text in `.txt` files, and another directory of associated metadata files (the RDF files). To make your life easier for the purpose of this tutorial, we have made a small python module called `gutenberg_cleanup` that has some handy functions for pulling out the relevant text and metadata out of the raw dataset. \n",
    "\n",
    "The [`gutenberg_cleanup`](gutenberg_cleanup.py) module contains several functions that can help with this: `get_filelist`, `read_file`, `get_gid`, `get_metadata` and `get_text`.\n",
    "\n",
    "They pretty much do the obvious: \n",
    "\n",
    "`get_gid` takes an html path and pulls out the book ID (`gid`)\n",
    "\n",
    "`get_metadata` takes a `gid` and returns a metadata object with various useful fields that will be used to create a unique key for each book\n",
    "\n",
    "`get_text` takes a path to an html file and returns the raw text extracted from HTML, cleaned of tags and punctuation and converted to lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a lookup table for the `.rdf` metadata files so we don't have to search the filesystem repeatedly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdf_lookup = {}\n",
    "find_gid = re.compile('(\\d+)')\n",
    "for root, dirs, files in os.walk(rdf_path):\n",
    "    for f in files:\n",
    "        name, ext = os.path.splitext(f)\n",
    "        if ext == '.rdf':\n",
    "            rdf_lookup[find_gid.findall(name)[0]] = os.path.join(root,f)\n",
    "rdf_lookup_b = sc.broadcast(rdf_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the raw dataset using `sc.parallelize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of books: 30807\n"
     ]
    }
   ],
   "source": [
    "filelist = gutenberg_cleanup.get_filelist(extract_path)\n",
    "\n",
    "print('Total number of books: %d'%len(filelist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `sc.parallelize` to distribute a dataset across the cluster, you can choose the number of partitions across which to distribute the dataset. The higher the number of partitions, the higher the \"parallelism\". When Spark subsequently executes maps and reduces on this dataset, it does so by dispatching tasks to different executors, which then request the cores under their control to do the actual work. By increasing the number of partitions, you increase the number of tasks - more tasks gives the Spark scheduler more flexibility in distributing the work across the cluster and therefore maximally leveraging the compute resources at its disposal. In some cases, where a single partition might require a lot of memory it can cause `Out of memory` errors - in such cases, simply reducing the amount of data per task by increasing the parallelism can help. \n",
    "\n",
    "Note that as long as the tasks take a few hundred milliseconds the scheduler should have no trouble dispatching them. On the other hand, there is a bit of overhead associated with partitioning the data so you don't want an unreasonably high number of partitions. You can see the [Spark guide](http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism) for a bit more detail. \n",
    "\n",
    "Below, we will choose to use 5 times as many partitions as we have cores in the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ncores = sc.defaultParallelism\n",
    "files_rdd = sc.parallelize(filelist, ncores*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rok/gutenberg_data/new_dload/1/0/0/0/10001/10001.txt',\n",
       " '/Users/rok/gutenberg_data/new_dload/1/0/0/0/10002/10002-8.txt',\n",
       " '/Users/rok/gutenberg_data/new_dload/1/0/0/0/10003/10003.txt',\n",
       " '/Users/rok/gutenberg_data/new_dload/1/0/0/0/10004/10004-8.txt',\n",
       " '/Users/rok/gutenberg_data/new_dload/1/0/0/0/10005/10005-8.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the list of filenames into a `key,value` pair RDD of metadata and text\n",
    "\n",
    "The raw Gutenberg Project dataset consists of `txt` files and files that hold metadata in XML format. For example, here are the first few lines of a metadata file at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      " <rdf:RDF xml:base=\"http://www.gutenberg.org/\"\n",
      "   xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\n",
      "   xmlns:dcam=\"http://purl.org/dc/dcam/\"\n",
      "   xmlns:cc=\"http://web.resource.org/cc/\"\n",
      "   xmlns:pgterms=\"http://www.gutenberg.org/2009/pgterms/\"\n",
      "   xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
      "   xmlns:dcterms=\"http://purl.org/dc/terms/\"\n",
      " >\n",
      "   <pgterms:ebook rdf:about=\"ebooks/1000\">\n",
      "     <dcterms:hasFormat>\n",
      "       <pgterms:file rdf:about=\"http://www.gutenberg.org/ebooks/1000.txt.utf-8\">\n",
      "         <dcterms:format>\n",
      "           <rdf:Description rdf:nodeID=\"N1d6fbe7c5c724eb9a80228a47d8a07c5\">\n",
      "             <dcam:memberOf rdf:resource=\"http://purl.org/dc/terms/IMT\"/>\n",
      "             <rdf:value rdf:datatype=\"http://purl.org/dc/terms/IMT\">text/plain</rdf:value>\n",
      "           </rdf:Description>\n",
      "         </dcterms:format>\n",
      "         <dcterms:isFormatOf rdf:resource=\"ebooks/1000\"/>\n",
      "         <dcterms:extent rdf:datatype=\"http://www.w3.org/2001/XMLSchema#integer\">600246</dcterms:extent>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/rok/gutenberg_data/cache/epub/1000/pg1000.rdf') as f: \n",
    "    print(' '.join(f.readlines()[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion procedure\n",
    "\n",
    "Our first task is to ingest this dataset by doing the following: \n",
    "\n",
    "1. open and read the text file\n",
    "2. match each metadata entry with its corresponding raw text \n",
    "3. produce an RDD of `(metadata, text)` pairs\n",
    "\n",
    "These steps are often very similar at the beginning of any analysis, and can be quite time consuming to get right. For the purposes of this exercise, we have already built the functions needed to perform these operations. They are found in [`gutenberg_cleanup.py`](gutenberg_cleanup.py) if you want to have a look. \n",
    "\n",
    "The important functions are:\n",
    "\n",
    "* `get_gid` -- returns the Gutenberg ID given filename\n",
    "* `get_metadata` -- return the metadata given an ID \n",
    "\n",
    "These will be used to construct a `key,value` pair RDD. The `key` will be the dictionary returned by `get_metadata`, while the `value` we will use the raw text returned by `get_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gutenberg_cleanup import get_metadata, clean_text, get_gid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the `gutenberg_cleanup` source file to the executors, we will use the `addPyFile` method of the `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('{cwd}/gutenberg_cleanup.py'.format(cwd=os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `map` method of `files_rdd` to map the filenames to `(metadata, text)` tuples using `get_gid` and `get_text` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd = (files_rdd.map(lambda filename: gutenberg_cleanup.read_file(filename, rdf_lookup_b.value))\n",
    "                     .filter(lambda x: x[0] is not None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we don't have to constantly re-load the data off disk, lets cache this RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#text_rdd.cache()\n",
    "ndocs = text_rdd.filter(lambda x: x[0] is not None).count()\n",
    "print('number of documents: ', ndocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_re = re.compile('[\\w\\']+')\n",
    "\n",
    "no_punctuation = re.compile(\"[^a-zA-Z0-9\\s'-]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "UI_url": {}
    }
   },
   "source": [
    "Since we called `count()`, it means that the entire RDD was generated/calculated. This combination of `cache` and `count` is a common way to check how much memory your dataset needs - once `count` completes you can check the memory taken up by the RDD by going to the \"Storage\" tag of the Spark UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is cached, next time you try to use `text_rdd` it will be much much quicker. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 ms, sys: 10 ms, total: 57 ms\n",
      "Wall time: 3.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assert(text_rdd.count() == ndocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw dataset to HDFS (or local storage)\n",
    "\n",
    "As a final bit of preparation before continuing with analysis, we save the raw data in a way that makes it faster to access later. We don't want to have to read the data off local disk every time we need to repeat some part of the analysis. Instead, it's much more advantageous to use the Hadoop Distributed File System (HDFS) to store the data once we've read it in and put it in a `key,value` format. \n",
    "\n",
    "By storing the data in HDFS, we make sure that the system can take advantage of data-locality at a later stage in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -XX:ParallelGCThreads=1\n",
      "15/11/20 15:00:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/11/20 15:00:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/roskarr/gutenberg/raw_text_rdd\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r -f /user/roskarr/gutenberg/raw_text_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_rdd.saveAsPickleFile('hdfs:///user/roskarr/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, whenever we need it, we can read the data off the HDFS instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_rdd = sc.pickleFile('hdfs:///user/roskarr/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 9 ms, total: 57 ms\n",
      "Wall time: 6.17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48177"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time text_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
