{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import sys, os, glob\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gutenberg N-Grams\n",
    "\n",
    "In this notebook, we will quantitatively explore the text of the [Gutenberg E-Books Project](https://www.gutenberg.org/), a free repository of e-books that are in the public domain. All of the English and German books have been downloaded for this tutorial and a small python package has been made available that allows you to easily parse the text and the associated metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing procedure\n",
    "\n",
    "1. Generate `(metadata,text)` RDD from raw data and save in HDFS\n",
    "2. create a metadata lookup table that can be distributed to the workers\n",
    "3. clean the dataset for missing values and repeated entries\n",
    "4. save the final, cleaned dataset to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the spark environment can be a bit of a trial and error procedure. Often you'll need to configure settings (in particular dealing with memory) to fit your cluster and your particular application. Below, we will specify a few of the most important ones -- but you can see the full list in the [Spark Configuration guide](http://spark.apache.org/docs/latest/configuration.html) and if you are using YARN there are critical options also listed under the [YARN deployment guide](http://spark.apache.org/docs/latest/running-on-yarn.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python environment setup\n",
    "\n",
    "First, we need to make sure that the `pyspark` libraries are accessible in this notebook. To do this, we can add them to the library search path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing `SparkConf` \n",
    "When starting the Spark runtime through the notebook or inside a script (i.e. when not calling one of the spark scripts like `spark_submit`), you can create a `SparkConf` object that allows you to set up the runtime. This is quite convenient and much more clean and readable than specifying the options on the commandline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the number of executors and cores into variables so we can refer to it later\n",
    "num_execs = 20\n",
    "exec_cores = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializing the SparkConf\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executor options\n",
    "\n",
    "The full list of options is very long, but the basic ones you'll *always* want to at least think about are ones pertaining to the basic configuration of the executors: number of executors, memory per executor, and number of cores per executor. \n",
    "\n",
    "A few notes about the memory configuration: the `spark.executor.memory` should not be set to the total memory of the node. Some memory is needed for the OS (including HDFS and other services), and still more is required for the Spark overhead. So in our case here, we have 16 Gb of memory per node but can only use around 12 Gb of this for the executors. Since we need to leave room for 10% YARN overhead, we specify 9 Gb here to be safe. If your executors start dying off for strange reasons, try reducing the memory here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b3fb5855110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executor.memory', '9g')\n",
    "conf.set('spark.executor.instances', str(num_execs))\n",
    "conf.set('spark.executor.cores', str(exec_cores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory configuration\n",
    "\n",
    "Two other key memory options exist, specifying the amount of executor memory reserved for *cached* data and for *shuffle* data. Depending on what your application is doing, you may need more of one or the other. For example, if you are running a lot of iterative operations on a large dataset, you probably want a good amount of memory for RDD caching. On the other hand, if you are doing lots of expensive shuffles that occur when sorting of grouping by key, you may want more shuffle memory. Note that if either one starts to run low, your application won't crash it will simply spill to disk. This usually isn't as bad as it sounds especially if the OS file cache kicks in. \n",
    "\n",
    "You can check on the cache memory and shuffle memory in two ways while your application is running. In the Spark UI, you can see the cached RDDs under the `Storage` tab - if they start spilling to disk, this is where you will see it. Similarly, if you are running a large shuffle job, you can click on the stage details in the Spark UI and see the shuffle memory and disk statistics. We will check on both of these later on in this application. \n",
    "\n",
    "Here we will set these two options explicitly for completeness, but actually keep the values at their defaults (60%  of the heap for caching, 20% for shuffles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b3fb5855110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.storage.memoryFraction', 0.6)\n",
    "conf.set('spark.shuffle.memoryFraction', 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver memory\n",
    "\n",
    "The amount of memory allocated to the driver program could be crucial if the driver has to deal with a lot of late-stage aggegation products or if you want to collect a significant chunk of data out of the RDD. You can see how much memory has been allocated to the driver either in the Spark Web UI or in the messages printed to the console at initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b3fb5855110>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.yarn.am.memory', '8g')\n",
    "conf.set('spark.yarn.am.cores', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PATH` and `PYTHONPATH`\n",
    "\n",
    "In some cases we need to tell the executors explicitly where the non-standard python libraries are located (this includes the spark libraries and seems to be new in Spark 1.4.0 -- a bug?). For this, we set the environment variable `PYTHONPATH`. Any other environment variable can be specified in this way, should it be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b3fb5855110>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executorEnv.PYTHONPATH', \n",
    "         '/cluster/apps/spark/spark-current/python:/cluster/apps/spark/spark-current/python/lib/py4j-0.8.2.1-src.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a custom python (miniconda), we also set the `PATH` explicitly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b3fb5855110>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executorEnv.PATH', os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the `SparkContext`\n",
    "This is our entry point to the Spark runtime - it is used to push data into spark or load RDDs from disk etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master = 'yarn-client', conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works successfully, you can check the [YARN application scheduler](http://hadoop.ethz.ch:8088/cluster) and you should see your app listed there. Clicking on the \"Application Master\" link will bring up the familiar Spark Web UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a key-value RDD of book metadata and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data into spark from a collection of local files is a very common task. A useful pattern to keep in mind is the following: \n",
    "\n",
    "1. make a list of filenames and distribute it among the workers\n",
    "3. \"map\" each filename to the data you want to get out\n",
    "4. now you are left with the RDD of raw data distributed among the workers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case of the Gutenberg Project e-book data, we have a directory of `html` files which hold the actual book text, and another directory of associated metadata files (the RDF files). To make your life easier for the purpose of this tutorial, we have made a small python module called `gutenberg_cleanup` that has some handy functions for pulling out the relevant text and metadata out of the raw dataset. \n",
    "\n",
    "The [`gutenberg_cleanup`](gutenberg_cleanup.py) module contains three functions that can help with this: `get_gid`, `get_metadata` and `get_text`.\n",
    "\n",
    "They pretty much do the obvious: \n",
    "\n",
    "`get_gid` takes an html path and pulls out the book ID (`gid`)\n",
    "\n",
    "`get_metadata` takes a `gid` and returns a metadata object with various useful fields that will be used to create a unique key for each book\n",
    "\n",
    "`get_text` takes a path to an html file and returns the raw text extracted from HTML, cleaned of tags and punctuation and converted to lower case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the raw dataset using `sc.parallelize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of books:  21469\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# get a list of all html files in the data directory\n",
    "flist = glob.glob('/cluster/work/sdid/roskarr/gutenberg/html/*html')\n",
    "print('number of books: ', len(flist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `sc.parallelize` to distribute a dataset across the cluster, you can choose the number of partitions across which to distribute the dataset. The higher the number of partitions, the higher the \"parallelism\". When Spark subsequently executes maps and reduces on this dataset, it does so by dispatching tasks to different executors, which then request the cores under their control to do the actual work. By increasing the number of partitions, you increase the number of tasks - more tasks gives the Spark scheduler more flexibility in distributing the work across the cluster and therefore maximally leveraging the compute resources at its disposal. In some cases, where a single partition might require a lot of memory it can cause `Out of memory` errors - in such cases, simply reducing the amount of data per task by increasing the parallelism can help. \n",
    "\n",
    "Note that as long as the tasks take a few hundred milliseconds the scheduler should have no trouble dispatching them. On the other hand, there is a bit of overhead associated with partitioning the data so you don't want an unreasonably high number of partitions. You can see the [Spark guide](http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism) for a bit more detail. \n",
    "\n",
    "Below, we will choose to use 5 times as many partitions as we have cores in the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_rdd = sc.parallelize(flist, num_execs*exec_cores*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cluster/work/sdid/roskarr/gutenberg/html/1000.html',\n",
       " '/cluster/work/sdid/roskarr/gutenberg/html/10000.html',\n",
       " '/cluster/work/sdid/roskarr/gutenberg/html/10001.html',\n",
       " '/cluster/work/sdid/roskarr/gutenberg/html/10002.html',\n",
       " '/cluster/work/sdid/roskarr/gutenberg/html/10003.html']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the list of filenames into a `key,value` pair RDD of metadata and text\n",
    "\n",
    "The raw Gutenberg Project dataset consists of HTML files and files that hold metadata in JSON format. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version='1.0' encoding='utf-8'?>\r",
      "\r\n",
      "<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.1//EN' 'http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd'>\r",
      "\r\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\">\r",
      "\r\n",
      "  <head><title> </title><meta http-equiv=\"Content-Style-Type\" content=\"text/css\"/><meta http-equiv=\"Content-Type\" content=\"application/xhtml+xml; charset=utf-8\"/><link rel=\"schema.DCTERMS\" href=\"http://purl.org/dc/terms/\"/>\r",
      "\r\n",
      "<link rel=\"schema.MARCREL\" href=\"http://id.loc.gov/vocabulary/relators/\"/>\r",
      "\r\n",
      "<meta content=\"The Magna Carta\" name=\"DCTERMS.title\"/>\r",
      "\r\n",
      "<meta content=\"http://www.gutenberg.orgfiles/10000/10000.txt\" name=\"DCTERMS.source\"/>\r",
      "\r\n",
      "<meta content=\"en\" scheme=\"DCTERMS.RFC4646\" name=\"DCTERMS.language\"/>\r",
      "\r\n",
      "<meta content=\"2015-04-04T04:40:30.599547+00:00\" scheme=\"DCTERMS.W3CDTF\" name=\"DCTERMS.modified\"/>\r",
      "\r\n",
      "<meta content=\"Public domain in the USA.\" name=\"DCTERMS.rights\"/>\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head /cluster/work/sdid/roskarr/gutenberg/html/10000.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n",
      "<rdf:RDF xml:base=\"http://www.gutenberg.org/\"\r\n",
      "  xmlns:cc=\"http://web.resource.org/cc/\"\r\n",
      "  xmlns:dcterms=\"http://purl.org/dc/terms/\"\r\n",
      "  xmlns:rdfs=\"http://www.w3.org/2000/01/rdf-schema#\"\r\n",
      "  xmlns:pgterms=\"http://www.gutenberg.org/2009/pgterms/\"\r\n",
      "  xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\r\n",
      "  xmlns:dcam=\"http://purl.org/dc/dcam/\"\r\n",
      ">\r\n",
      "  <pgterms:ebook rdf:about=\"ebooks/10000\">\r\n"
     ]
    }
   ],
   "source": [
    "!head /cluster/work/sdid/roskarr/gutenberg/rdf-files/10000/pg10000.rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion procedure\n",
    "\n",
    "Our first task is to ingest this dataset by doing the following: \n",
    "\n",
    "1. convert the html into raw text\n",
    "2. deal with special characters, HTML tags, etc.\n",
    "3. match each metadata entry with its corresponding raw text and compose tuples of the type (metadata, text)\n",
    "\n",
    "\n",
    "These three steps are often the first step of any analysis, and can be quite time consuming to get right. For the purposes of this exercise, we have already built the functions needed to perform these operations. They are found in [`gutenberg_cleanup.py`](gutenberg_cleanup.py) if you want to have a look. \n",
    "\n",
    "The important functions are:\n",
    "\n",
    "* `get_gid` -- returns the Gutenberg ID given an html file\n",
    "* `get_text` -- get cleaned, raw text out of an html file\n",
    "* `get_metadata` -- return the metadata given an ID \n",
    "\n",
    "These will be used to construct a `key,value` pair RDD. The `key` will be the dictionary returned by `get_metadata`, while the `value` we will use the raw text returned by `get_text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gutenberg_cleanup\n",
    "from gutenberg_cleanup import get_metadata, get_text, get_gid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass the `gutenberg_cleanup` source file to the executors, we will use the `addPyFile` method of the `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('{cwd}/gutenberg_cleanup.py'.format(cwd=os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `map` method of `files_rdd` to map the filenames to `(metadata, text)` tuples using `get_gid` and `get_text` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "id_text_rdd = files_rdd.map(lambda filename: (get_gid(filename), get_text(filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have (ID, text), and we need to make another `map` to get (`metadata, text`) tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "text_rdd = (id_text_rdd.map(lambda (ID, text): (get_metadata(ID), text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we don't have to constantly re-load the data off disk, lets cache this RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 ms, sys: 9 ms, total: 70 ms\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_rdd.cache()\n",
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "UI_url": {}
    }
   },
   "source": [
    "Since we called `count()`, it means that the entire RDD was generated/calculated. This combination of `cache` and `count` is a common way to check how much memory your dataset needs - once `count` completes you can check the memory taken up by the RDD by going to the \"Storage\" tag of the Spark UI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is cached, next time you try to use `text_rdd` it will be much much quicker. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 ms, sys: 8 ms, total: 45 ms\n",
      "Wall time: 25.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21469"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert(_ == 15081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, we could call the native python `map` in exactly the same way (and run it on the local machine only), though this would take much longer to complete, i.e. \n",
    "\n",
    "    text = map(lambda f: (get_metadata(get_gid(f)), get_text(f)), flist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the raw dataset to HDFS (or local storage)\n",
    "\n",
    "As a final bit of preparation before continuing with analysis, we save the raw data in a way that makes it faster to access later. We don't want to have to read the data off local disk every time we need to repeat some part of the analysis. Instead, it's much more advantageous to use the Hadoop Distributed File System (HDFS) to store the data once we've read it in and put it in a `key,value` format. \n",
    "\n",
    "By storing the data in HDFS, we make sure that the system can take advantage of data-locality at a later stage in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "text_rdd.saveAsPickleFile('hdfs:///user/roskarr/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, whenever we need it, we can read the data off the HDFS instead: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "text_rdd = sc.pickleFile('hdfs:///user/roskarr/gutenberg/raw_text_rdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 ms, sys: 9 ms, total: 53 ms\n",
      "Wall time: 6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21469"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time text_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this read time to nearly two minutes it took to read it off the local filesystem initially. If you look at the details for this stage in the Spark UI you can understand why this is: in the column named \"locality level\", you see that for many tasks it says `NODE LOCAL` while for others it might say `RACK LOCAL`. These mean that either the data chunk was physically present on the disk of the node that was reading it in (`NODE LOCAL`) or it was on one of the nodes on the same switch (`RACK LOCAL`). Of course the additional advantage is not having to deal with the filesystem overhead of 10k+ small files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at what this RDD looks like -- this is often most easily done using the `first` method of the RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata:  {'lang': u'en', 'first_name': None, 'last_name': None, 'title': '- No Title -', 'downloads': u'243', 'death_year': None, 'gid': 1000, 'birth_year': None}\n",
      "\n",
      "text:  pageno position absolute right font medium sansserif textindent pagenoafter color gray content attrtitle lineno position absolute left font medium sansserif textindent linenoafter color gray content attrtitle tocpageref float right pre fontfamily monospace fontsize em whitespace prewrap in italian with no accentsbit text please see my notes about various versions beneath this header copyright laws are changing all over the world be sure to check the copyright laws for your country before posting these files please take a look at the important information in this header we encourage you to keep this file on your own disk keeping an electronic path open for the next readers do not remove this information on contacting project gutenberg to get etexts and further information is included below we need your donations this file should be named ddcdtxt or ddcdzip versions based on separate sources get new letter ddcdatxt we are now trying to release all our books one month in advance of the of\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "meta, text = text_rdd.first()\n",
    "print('metadata: ', meta)\n",
    "print('\\ntext: ', text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `key` is now a dictionary, and the `value` is the text, as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First bit of analysis -- how long are the documents? \n",
    "\n",
    "Lets have a quick look at the number of words in these books. We'll do this by taking the values of the `text_rdd` (i.e. the text), use the `split` method of the text string to break it up into a list of words, and then use the built-in `len` method to get the length of this sequence. Finally, we will use the `collect` method to extract the numbers to the driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "text_lengths = (text_rdd.values()\n",
    "                        .map(lambda text: len(text.split()))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#assert(text_lengths[:10] == [66976, 17180, 8455, 53505, 65001, 50678, 72798, 16377, 30705, 68609])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAGnCAYAAACThhA/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3fcce64a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(text_lengths, bins = 100, range=[0,3e5]);\n",
    "plt.xlabel('$N_{words}$'); plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variables: a good way to distribute metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we manipulate the dataset, we will want ready access to the metadata, but at the moment we have no efficient way of working with it. This is because it is either\n",
    "\n",
    "a) on disk in many thousands of small files (killing the filesystem and making HPC admins furious)\n",
    "\n",
    "*or*\n",
    "\n",
    "b) a part of the RDD itself \n",
    "\n",
    "So if we want to look at just the metadata, we need to either read it off the disk (bad) or extract it from the RDD every time (expensive). \n",
    "\n",
    "For these kinds of situations when passing around some data to all the workers is required, you should use Spark's [broadcast variable](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables) mechanism. By placing data inside a broadcast variable, you make it available to all the workers and it only needs to be sent across the network once. The next time a value from the broadcast variable is needed, it's simply taken from the local executor's memory, incurring no extra network cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the list of fields\n",
    "metadata_fields = text_rdd.first()[0].keys() # this takes the metadata of the first element and extracts the keys\n",
    "meta_dict = dict()\n",
    "\n",
    "for meta in text_rdd.keys().collect() :\n",
    "    meta_dict[meta['gid']] = {key: meta[key] for key in metadata_fields}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a look-up table that allows us to quickly access all the metadata indexed by `gid`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'birth_year': u'1954',\n",
       " 'death_year': None,\n",
       " 'downloads': u'352',\n",
       " 'first_name': u'Bruce',\n",
       " 'gid': 101,\n",
       " 'lang': u'en',\n",
       " 'last_name': u'Sterling',\n",
       " 'title': u'The Hacker Crackdown: Law and Disorder on the Electronic Frontier'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get, for example, the author birth year for book with `gid = 101`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1954'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_dict[101]['birth_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the broadcast variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call it meta_b for 'broadcast'\n",
    "meta_b = sc.broadcast(meta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying data object stored in `meta_b` can be accessed simply by\n",
    "\n",
    "    > meta_b.value\n",
    "    \n",
    "We'll make use of this soon. If you check the console output, you will see an INFO message that the broadcast has been created, i.e. \n",
    "\n",
    "```\n",
    "15/06/24 17:18:44 INFO storage.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 910.7 KB, free 4.1 GB)\n",
    "15/06/24 17:18:44 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.201.20.22:47821 (size: 910.7 KB, free: 4.1 GB)\n",
    "15/06/24 17:18:44 INFO spark.SparkContext: Created broadcast 6 from broadcast at PythonRDD.scala:403\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the metadata dictionary for later use\n",
    "We will need the metadata dictionary at a later point, so we save it to disk now to avoid having to regenerate it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cPickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump(meta_dict, open('{home}/gutenberg_metadata.dump'.format(home=os.environ['HOME']), 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data with filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to do some quality checks on the data. Let's check out the first couple of metadata entries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': u'243',\n",
       "  'first_name': None,\n",
       "  'gid': 1000,\n",
       "  'lang': u'en',\n",
       "  'last_name': None,\n",
       "  'title': '- No Title -'},\n",
       " {'birth_year': None,\n",
       "  'death_year': None,\n",
       "  'downloads': u'269',\n",
       "  'first_name': None,\n",
       "  'gid': 10000,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Anonymous',\n",
       "  'title': u'The Magna Carta'},\n",
       " {'birth_year': u'1863',\n",
       "  'death_year': u'1950',\n",
       "  'downloads': u'274',\n",
       "  'first_name': u'Lucius Annaeus',\n",
       "  'gid': 10001,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Seneca',\n",
       "  'title': u'Apocolocyntosis'},\n",
       " {'birth_year': u'1877',\n",
       "  'death_year': u'1918',\n",
       "  'downloads': u'865',\n",
       "  'first_name': u'William Hope',\n",
       "  'gid': 10002,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Hodgson',\n",
       "  'title': u'The House on the Borderland'},\n",
       " {'birth_year': u'1833',\n",
       "  'death_year': u'1923',\n",
       "  'downloads': u'15',\n",
       "  'first_name': u'Mary King',\n",
       "  'gid': 10003,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Waddington',\n",
       "  'title': u'My First Years as a Frenchwoman, 1876-1879'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at just the first few entries it becomes clear that we're going to have to do some quality control here. For example, we probably don't want books with \"None\" as either of the author names, and likewise we have to have the birth date in order to be able to create a time series out of the data in the end. \n",
    "\n",
    "Construct an RDD, as above, except that you filter out all the elements that have `None` for `title`, `first_name`, `last_name`, or `birth_year`. In addition, filter out the data with \"BC\" in either birth or death year. \n",
    "\n",
    "As a reminder, here is a cartoon illustration of the difference between `map` and `filter` RDD methods. `map` simply applies the function to each element, returning another element. \n",
    "\n",
    "![map](../figs/map_example.svg)\n",
    "\n",
    "In this example, with `filter` we are filtering out all the even elements of the RDD. The function that is passed to `filter` just has to evaluate to either `True` (1) or `False` (0) given the input data. The function `lambda (k,v): v%2` evaluates to 0 if `v` is even and 1 of `v` is odd. Hence, only the odd values pass the filter. \n",
    "\n",
    "![filter](../figs/filter_example.svg)\n",
    "\n",
    "The `filter_func` has already been defined for you below, but you need to apply it to `text_rdd`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_func(meta) : \n",
    "    no_none = all([meta[name] is not None for name in ['title', 'first_name', 'last_name', 'birth_year']])\n",
    "    if not no_none : \n",
    "        return False\n",
    "    else : \n",
    "        no_birth_bc = 'BC' not in meta['birth_year']\n",
    "        no_death_bc = True if meta['death_year'] is None else 'BC' not in meta['death_year']\n",
    "        return no_birth_bc + no_death_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "filtered_rdd = text_rdd.filter(lambda (meta, text): filter_func(meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'birth_year': u'1863',\n",
       "  'death_year': u'1950',\n",
       "  'downloads': u'274',\n",
       "  'first_name': u'Lucius Annaeus',\n",
       "  'gid': 10001,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Seneca',\n",
       "  'title': u'Apocolocyntosis'},\n",
       " {'birth_year': u'1877',\n",
       "  'death_year': u'1918',\n",
       "  'downloads': u'865',\n",
       "  'first_name': u'William Hope',\n",
       "  'gid': 10002,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Hodgson',\n",
       "  'title': u'The House on the Borderland'},\n",
       " {'birth_year': u'1833',\n",
       "  'death_year': u'1923',\n",
       "  'downloads': u'15',\n",
       "  'first_name': u'Mary King',\n",
       "  'gid': 10003,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Waddington',\n",
       "  'title': u'My First Years as a Frenchwoman, 1876-1879'},\n",
       " {'birth_year': u'1864',\n",
       "  'death_year': u'1948',\n",
       "  'downloads': u'9',\n",
       "  'first_name': u'Anna Robertson Brown',\n",
       "  'gid': 10004,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Lindsay',\n",
       "  'title': u'The Warriors'},\n",
       " {'birth_year': u'1775',\n",
       "  'death_year': u'1861',\n",
       "  'downloads': u'17',\n",
       "  'first_name': u'George',\n",
       "  'gid': 10005,\n",
       "  'lang': u'en',\n",
       "  'last_name': u'Tucker',\n",
       "  'title': u'A Voyage to the Moon\\r'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rdd.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many do we have left? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of books after filtering:  16227\n"
     ]
    }
   ],
   "source": [
    "nfiltered = filtered_rdd.count()\n",
    "print('number of books after filtering: ', nfiltered)\n",
    "#assert(nfiltered == 11872)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final bit of cleanup: \n",
    "\n",
    "some of the books end up split across multiple entries. Since it's the same book, each of the entries should have the same `gid`. \n",
    "\n",
    "To check for this we will use one of the most basic and common MapReduce patterns -- the key count: \n",
    "\n",
    "* map the data into `key`,`value` pairs where `key` is the quantity we want to count and `value` is just 1. In this case, the `key` will be `gid`\n",
    "* invoke a reduction *by key*, where the reduction operator is a simple addition\n",
    "\n",
    "Finally, we will sort the result in descending order and print out the first few elements to check whether we have to worry about documents spanning multiple files or not. \n",
    "\n",
    "The RDD operations that are needed are [`reduceByKey`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) and [sortBy](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy).\n",
    "\n",
    "`reduceByKey` works by grouping all data of a key together and applying the reduction function just to that data. Here's a simple illustration, in this case using a simple addition of two elements as a reduction:\n",
    "\n",
    "![reducebykey](../figs/reduceByKey_example.svg)\n",
    "\n",
    "\n",
    "\n",
    "For the `keyFunc` of the call to `sortBy`, use a `lambda` function that extracts the counts obtained from the `reduceByKey`. \n",
    "\n",
    "So, the procedure should be : \n",
    "\n",
    "1. `map` the `filtered_rdd` using a lambda function to contain (`gid`, 1) tuples\n",
    "2. `reduceByKey`\n",
    "3. `sortBy` (specify decreasing order, see the API) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6478, 43),\n",
       " (3772, 40),\n",
       " (8700, 35),\n",
       " (3332, 33),\n",
       " (12233, 29),\n",
       " (3425, 23),\n",
       " (2440, 16),\n",
       " (6475, 15),\n",
       " (12145, 9),\n",
       " (12383, 7)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FILL IN: map the filtered_rdd to contain just the tuple (gid, 1)\n",
    "map_filtered = filtered_rdd.map(lambda (meta, text): (meta['gid'],1))\n",
    "\n",
    "# reduce the map_filtered rdd by key to get the total counts per gid\n",
    "reduced_gid_rdd = map_filtered.reduceByKey(add)\n",
    "\n",
    "# sort by count and print out the top 10\n",
    "reduced_gid_rdd.sortBy(lambda (key, count): count, False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assert(_ == [(6478, 43), (3772, 40), (8700, 35), (3332, 33), (12233, 29), (3425, 23), (2440, 16), (6475, 15), (12145, 9), (12383, 7)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are several transformations here that lead to the final result, `sorted_reduced`. A common syntax is to group them all together, by enclosing them in `( )` and chaining them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6478, 43),\n",
       " (3772, 40),\n",
       " (8700, 35),\n",
       " (3332, 33),\n",
       " (12233, 29),\n",
       " (3425, 23),\n",
       " (2440, 16),\n",
       " (6475, 15),\n",
       " (12145, 9),\n",
       " (12383, 7)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "(filtered_rdd.map(lambda (meta, text): (meta['gid'], 1))\n",
    "             .reduceByKey(add)\n",
    "             .sortBy(lambda (key,count): count, False)\n",
    "             .take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a few that are made up of multiple sections. To combine them, we will use `reduceByKey` which will result in having an RDD of `gid`'s as keys and the combined text of each `gid`. The reduction function in `reduceByKey` can be a simple in-line function that just adds two elements together (but can't be the `add` function because that expects the arguments to be numbers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_rdd = (filtered_rdd.map(lambda (meta, text): (meta['gid'], text))\n",
    "                           .reduceByKey(lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple sanity check, lets look at `gid`=6478, which according to the cell above has 43 sections in the original dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_rdd.map(lambda (meta, text): (meta['gid'],1))\n",
    "                .lookup(6478))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_rdd.lookup(6478))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to do all these pre-processing steps again at a later point, lets also save the `cleaned_rdd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_rdd.saveAsPickleFile('/user/roskarr/gutenberg/cleaned_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now saved in the directory we specified, one file per partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked up _JAVA_OPTIONS: -XX:ParallelGCThreads=1\n",
      "15/09/04 09:32:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 401 items\n",
      "-rw-r--r--   3 roskarr supergroup          0 2015-09-04 09:32 /user/roskarr/gutenberg/cleaned_rdd/_SUCCESS\n",
      "-rw-r--r--   3 roskarr supergroup   63571800 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00000\n",
      "-rw-r--r--   3 roskarr supergroup   19921938 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00001\n",
      "-rw-r--r--   3 roskarr supergroup   13487504 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00002\n",
      "-rw-r--r--   3 roskarr supergroup   15221090 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00003\n",
      "-rw-r--r--   3 roskarr supergroup   17893803 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00004\n",
      "-rw-r--r--   3 roskarr supergroup   18726897 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00005\n",
      "-rw-r--r--   3 roskarr supergroup   14910372 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00006\n",
      "-rw-r--r--   3 roskarr supergroup   17157664 2015-09-04 09:31 /user/roskarr/gutenberg/cleaned_rdd/part-00007\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /user/roskarr/gutenberg/cleaned_rdd | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we used the `hadoop` command in the local bash shell (the `!` at the beginning of the line means we are executing the command in the shell). This allows us to access the hadoop filesystem (HDFS), which is separate from the local file system we are used to. You'll notice, for example, that this directory doesn't exist in the local filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access /user/roskarr/gutenberg/: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls /user/roskarr/gutenberg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also browse the filesystem via the [HDFS web UI](http://hadoop.ethz.ch:50070). The `hadoop fs` command has many of the same options as regular Linux/Unix shell commands you might be used to for manipulating files and directories. Try running\n",
    "\n",
    "```bash\n",
    "cluster $> module load hadoop\n",
    "cluster $> hadoop fs -help\n",
    "```\n",
    "\n",
    "in a new shell to see all the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of steps up until this point\n",
    "\n",
    "We've done quite a lot already with our dataset in Spark, although it's only the beginning!\n",
    "\n",
    "1. created an RDD of filenames (`filename_rdd`)\n",
    "2. transformed the `filename_rdd` into an RDD of `(metadata, text)` (`text_rdd`); we also saved this to HDFS\n",
    "3. filtered out data with bad metadata, e.g. missing author names etc.\n",
    "3. cleaned up the entries a bit more by merging ones with identical IDs; we called this `cleaned_rdd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutting down the `SparkContext`\n",
    "\n",
    "Now that the pre-processing is done, we will shut down the `SparkContext` before continuing to the data analysis notebook. We have all of our results saved in HDFS, so to continue from where we left off will just require loading data from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the pre-processing steps are complete, we can continue to the [analysis notebook](part2-ngram-viewer-SOLUTIONS.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
